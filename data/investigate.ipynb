{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "\n",
    "save_names = ['llava_pretrain']\n",
    "data = {name: [] for name in save_names}\n",
    "file_path = '/home/mila/l/le.zhang/scratch/datasets/Cambrian-Alignment/jsons/alignment_2.5m.jsonl'\n",
    "base_path = '/home/mila/l/le.zhang/scratch/datasets/Cambrian-Alignment'\n",
    "\n",
    "# Count the total number of lines for the progress bar\n",
    "total_lines = sum(1 for _ in open(file_path))\n",
    "\n",
    "with jsonlines.open(file_path) as reader:\n",
    "    for entry in tqdm(reader, total=total_lines, desc=\"Processing entries\"):\n",
    "        data_split = entry['image'].split('/')[1]\n",
    "        \n",
    "        if data_split in save_names:\n",
    "            if data_split == 'llava_pretrain':\n",
    "                entry['image'] = entry['image'].replace('llava_pretrain/images', 'sbu558k')\n",
    "            image_path = os.path.join(base_path, entry['image'])\n",
    "            image_path = os.path.normpath(image_path)\n",
    "            \n",
    "            if os.path.exists(image_path):\n",
    "                data[data_split].append(entry)\n",
    "            else:\n",
    "                break\n",
    "# count the number of each dataset\n",
    "for key, entries in data.items():\n",
    "    print(f\"{key}: {len(entries)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/home/mila/l/le.zhang/scratch/datasets/Cambrian-Alignment/sbu558k/00120/001200924.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as json for each data\n",
    "for key, entries in data.items():\n",
    "    save_path = f'/home/mila/l/le.zhang/scratch/datasets/Cambrian-Alignment/jsons/{key}.json'\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(entries, f)\n",
    "    print(f\"Saved {len(entries)} entries to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "def get_linear_input(sequence_output):\n",
    "    cls_token = sequence_output[:, 0]\n",
    "    patch_tokens = sequence_output[:, 1:]\n",
    "    linear_input = torch.cat([cls_token, patch_tokens.mean(dim=1)], dim=1)\n",
    "    return linear_input\n",
    "directory = '/home/mila/l/le.zhang/scratch/light_align/data/image_embedding/dinov2-base'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".pt\"):\n",
    "        print(os.path.join(directory, filename))\n",
    "        image_embedding = torch.load(os.path.join(directory, filename))\n",
    "        image_embedding = get_linear_input(image_embedding)\n",
    "        torch.save(image_embedding, os.path.join(directory, filename))\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "def compare_random_tensor_value(pt_file1, pt_file2, position=None):\n",
    "    # 加载 .pt 文件\n",
    "    tensor1 = torch.load(pt_file1,weights_only=True)\n",
    "    tensor2 = torch.load(pt_file2,weights_only=True)\n",
    "    # 如果没有指定位置，随机选择一个索引位置\n",
    "    if position is None:\n",
    "        position = random.randint(0, len(tensor1) - 1) \n",
    "    # 获取指定位置的值\n",
    "    value1 = tensor1[position]\n",
    "    value2 = tensor2[position]\n",
    "    \n",
    "    # 比较两个值并打印结果\n",
    " \n",
    "    print(f\"The values at position {position} of index {index} are different.\\n\"\n",
    "              f\"File 1 value: {value1}\\nFile 2 value: {value2}\")\n",
    "    \n",
    "    # return value1, value2\n",
    "\n",
    "index = random.randint(150,200)\n",
    "pt_file1 = f\"/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/text_embedding/gte-large-en-v1.5/dreamclipcc3m_raw_caption/{index}.pt\"\n",
    "pt_file2 = f\"/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/text_embedding/gte-large-en-v1.5/dreamclipcc3m_raw/{index}.pt\"\n",
    "compare_random_tensor_value(pt_file1,pt_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.randint(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from natsort import natsorted\n",
    "path_list = ['/home/mila/l/le.zhang/scratch/light_align/data/image_embedding/dinov2-large/ALLaVAVFLAN', '/home/mila/l/le.zhang/scratch/light_align/data/image_embedding/dinov2-large/LLaVA558K']\n",
    "all_files = []\n",
    "for dir_path in path_list:\n",
    "    files = glob.glob(os.path.join(dir_path, \"*.pt\"))\n",
    "    sorted_files = natsorted(files)\n",
    "    all_files.extend(sorted_files)\n",
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "# read json\n",
    "with open('/home/mila/l/le.zhang/scratch/datasets/Cambrian-Alignment/jsons/llava_pretrain.json', 'r') as f:\n",
    "    sharegpt4v = json.load(f)\n",
    "with open('/home/mila/l/le.zhang/scratch/datasets/Cambrian-Alignment/jsons/sbu558k.json', 'r') as f:\n",
    "    sbu558k = json.load(f)\n",
    "with open('/home/mila/l/le.zhang/scratch/datasets/Cambrian-Alignment/jsons/coco.json', 'r') as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to caculate avg length \n",
    "def avg_length(data):\n",
    "    avg = 0\n",
    "    for entry in data:\n",
    "        avg += len(entry['conversations'][-1]['value'])\n",
    "    return avg/len(data)\n",
    "\n",
    "avg_length(sharegpt4v), avg_length(sbu558k), avg_length(coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharegpt4v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from natsort import natsorted\n",
    "import glob\n",
    "\n",
    "def merge_pt_files(directory):\n",
    "    # 获取所有的 .pt 文件路径，并按名字顺序排序\n",
    "    files = glob.glob(os.path.join(directory, \"*.pt\"))\n",
    "    files = natsorted(files)\n",
    "    \n",
    "    # 确保文件数量为偶数\n",
    "    if len(files) % 2 != 0:\n",
    "        raise ValueError(\"The number of .pt files is not even. Please make sure the files can be paired.\")\n",
    "\n",
    "    # 遍历所有文件，按两个为一组进行合并\n",
    "    for i in range(0, len(files), 2):\n",
    "        file1 = files[i]\n",
    "        file2 = files[i+1]\n",
    "        \n",
    "        # 加载两个文件\n",
    "        tensor1 = torch.load(file1)\n",
    "        tensor2 = torch.load(file2)\n",
    "        \n",
    "        # 在第一个维度进行拼接\n",
    "        merged_tensor = torch.cat((tensor1, tensor2), dim=0)\n",
    "        \n",
    "        # 生成新文件名\n",
    "        base_name1 = os.path.basename(file1).replace(\".pt\", \"\")\n",
    "        base_name2 = os.path.basename(file2).replace(\".pt\", \"\")\n",
    "        new_file_name = f\"{base_name1}_{base_name2}.pt\"\n",
    "        new_file_path = os.path.join(directory, new_file_name)\n",
    "        \n",
    "        # 保存拼接后的tensor\n",
    "        torch.save(merged_tensor, new_file_path)\n",
    "        print(f\"Saved merged tensor to {new_file_path}\")\n",
    "\n",
    "# 示例用法\n",
    "# merge_pt_files('/path/to/your/directory')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "file_dir = \"/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/text_embedding/gte-large-en-v1.5/laion30m_caption\"\n",
    "files = glob.glob(os.path.join(file_dir, \"*.pt\"))\n",
    "total_len = 0\n",
    "for file in tqdm(files):\n",
    "    tensor = torch.load(file, map_location=torch.device('cpu'), weights_only=False)\n",
    "    total_len += tensor.size(0)\n",
    "    if tensor.size(0) != 4096:\n",
    "        print(file, tensor.size(0))\n",
    "print(total_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1880/1880 [03:54<00:00,  8.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/text_embedding/gte-large-en-v1.5/dreamclipcc12mhf_raw_caption/1879.pt 3333\n",
      "7699717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "file_dir = \"/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/text_embedding/gte-large-en-v1.5/dreamclipcc12mhf_raw_caption\"\n",
    "files = glob.glob(os.path.join(file_dir, \"*.pt\"))\n",
    "total_len = 0\n",
    "for file in tqdm(files):\n",
    "    tensor = torch.load(file, map_location=torch.device('cpu'), weights_only=False)\n",
    "    total_len += tensor.size(0)\n",
    "    if tensor.size(0) != 4096:\n",
    "        print(file, tensor.size(0))\n",
    "print(total_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_580019/900397271.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vision_embedding = torch.load('/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/text_embedding/gte-large-en-v1.5/laion30m_caption/0.pt')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "vision_embedding = torch.load('/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/text_embedding/gte-large-en-v1.5/laion30m_caption/0.pt')\n",
    "print(vision_embedding.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "file_dir = \"/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-base/dreamclipcc12mhf\"\n",
    "files = glob.glob(os.path.join(file_dir, \"*.pt\"))\n",
    "total_len = 0\n",
    "for file in tqdm(files):\n",
    "    tensor = torch.load(file, map_location=torch.device('cpu'), weights_only=False)\n",
    "    total_len += tensor.size(0)\n",
    "    if tensor.size(0) != 1024:\n",
    "        print(file, tensor.size(0))\n",
    "print(total_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "file_dir = \"/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-giant/dreamclipcc3m\"\n",
    "files = glob.glob(os.path.join(file_dir, \"*.pt\"))\n",
    "total_len = 0\n",
    "for file in tqdm(files):\n",
    "    tensor = torch.load(file, map_location=torch.device('cpu'), weights_only=False)\n",
    "    total_len += tensor.size(0)\n",
    "    if tensor.size(0) != 1024:\n",
    "        print(file, tensor.size(0))\n",
    "print(total_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_config import DATADIR\n",
    "# load csv file\n",
    "csv = pd.read_csv(DATADIR['dreamclipcc12mhf']['annotation'], nrows=100000)\n",
    "csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open image and show\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "# 指定index为5的行是我们要显示的图像\n",
    "index = random.randint(0, 100000)\n",
    "image_path = os.path.join(DATADIR['dreamclipcc12mhf']['imagedir'], csv.iloc[index]['Image Path'])\n",
    "\n",
    "# 打开图像并显示 标题为caption\n",
    "image = Image.open(image_path)\n",
    "plt.imshow(image)\n",
    "plt.title(\"\\n\".join(csv.iloc[index]['longLLA_captions'].split(\" \")), loc='left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: test if embedding are encoded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Url</th>\n",
       "      <th>raw_caption</th>\n",
       "      <th>shortIB_captions</th>\n",
       "      <th>longIB_captions</th>\n",
       "      <th>shortSV_captions</th>\n",
       "      <th>longSV_captions</th>\n",
       "      <th>shortLLA_captions</th>\n",
       "      <th>longLLA_captions</th>\n",
       "      <th>Image Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://1.bp.blogspot.com/_dWmIOlGB7_0/S_HA9F1R...</td>\n",
       "      <td>this watch rolex gmt master has an independent...</td>\n",
       "      <td>a rolex watch with red and black dials</td>\n",
       "      <td>The image showcases a stunning silver and red ...</td>\n",
       "      <td>A Rolex watch with a black face and red and wh...</td>\n",
       "      <td>The image showcases a Rolex Submariner wristwa...</td>\n",
       "      <td>A silver and black watch with a red second hand.</td>\n",
       "      <td>The image features a silver and black wristwat...</td>\n",
       "      <td>CC12M_HF/images/0000000/0000000.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://2.bp.blogspot.com/-TwtZYTU4e9I/UQmEywkl...</td>\n",
       "      <td>picture of white modern bathroom furniture as ...</td>\n",
       "      <td>a white toilet sitting next to a sink in a bat...</td>\n",
       "      <td>The image showcases a modern and stylish bathr...</td>\n",
       "      <td>A bathroom with a white toilet, sink, and mirror.</td>\n",
       "      <td>The image captures a modern bathroom, bathed i...</td>\n",
       "      <td>A bathroom with a toilet and sink.</td>\n",
       "      <td>The image depicts a modern bathroom with a whi...</td>\n",
       "      <td>CC12M_HF/images/0000000/0000002.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://2.bp.blogspot.com/--K8LdRsj99U/VQaOjr33...</td>\n",
       "      <td>the american cowboy chronicles native american...</td>\n",
       "      <td>a close up of a brown bear with its mouth open</td>\n",
       "      <td>The image features a large brown bear with its...</td>\n",
       "      <td>A close up of a bear with its mouth wide open.</td>\n",
       "      <td>The image captures a close-up view of a brown ...</td>\n",
       "      <td>A close up of a brown bear with its mouth open.</td>\n",
       "      <td>The image features a large brown bear with its...</td>\n",
       "      <td>CC12M_HF/images/0000000/0000003.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://2.bp.blogspot.com/-MBHbOy37tvU/TaX19Rrr...</td>\n",
       "      <td>assimilation process in the indian style and f...</td>\n",
       "      <td>a woman in a pink sari sitting on a bed</td>\n",
       "      <td>In the picture, there are visible objects such...</td>\n",
       "      <td>A woman in a long pink dress sitting on a bed.</td>\n",
       "      <td>In the image, a woman is the central figure, s...</td>\n",
       "      <td>A woman is sitting on a bed with a pink dress.</td>\n",
       "      <td>The image features a beautiful woman wearing a...</td>\n",
       "      <td>CC12M_HF/images/0000000/0000004.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://2.bp.blogspot.com/-2HQcTDQ4oQs/V1oBoudL...</td>\n",
       "      <td>in mourning afterglow album lyrics in mourning...</td>\n",
       "      <td>a painting of a lighthouse in the middle of a ...</td>\n",
       "      <td>The painting depicts a lighthouse standing on ...</td>\n",
       "      <td>A painting of a rocky island in the middle of ...</td>\n",
       "      <td>The image presents a dramatic scene of a rocky...</td>\n",
       "      <td>A painting of a lighthouse on a rocky island s...</td>\n",
       "      <td>The image depicts a dramatic scene of a lighth...</td>\n",
       "      <td>CC12M_HF/images/0000000/0000005.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7699712</th>\n",
       "      <td>https://www.wtravelmagazine.com/wp-content/upl...</td>\n",
       "      <td>person on a yacht in cannes</td>\n",
       "      <td>a man and woman standing on a boat in the water</td>\n",
       "      <td>In the image, there is a man and a woman posin...</td>\n",
       "      <td>person on a yacht in cannes</td>\n",
       "      <td>In the image, a man and a woman are standing o...</td>\n",
       "      <td>A man and a woman are standing on a boat.</td>\n",
       "      <td>The image features a man and a woman standing ...</td>\n",
       "      <td>CC12M_HF/images/0001021/0005648.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7699713</th>\n",
       "      <td>https://www.writeups.org/wp-content/uploads/Or...</td>\n",
       "      <td>orko he man and the masters of the universe ca...</td>\n",
       "      <td>two cartoon characters wearing hats and holdin...</td>\n",
       "      <td>In the picture, there are two cartoon characte...</td>\n",
       "      <td>orko he man and the masters of the universe ca...</td>\n",
       "      <td>In the image, we see a scene from the animated...</td>\n",
       "      <td>Two cartoon characters in witch costumes are s...</td>\n",
       "      <td>The image features two cartoon characters, bot...</td>\n",
       "      <td>CC12M_HF/images/0001021/0005649.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7699714</th>\n",
       "      <td>https://www.wubbanub.com/wp-content/uploads/ca...</td>\n",
       "      <td>old school ways to tell if youre having a boy ...</td>\n",
       "      <td>a boy and a girl holding hands walking down th...</td>\n",
       "      <td>The image captures two young children, a boy a...</td>\n",
       "      <td>old school ways to tell if youre having a boy ...</td>\n",
       "      <td>In the image, two children are seen walking do...</td>\n",
       "      <td>A boy and a girl are walking down a sidewalk h...</td>\n",
       "      <td>The image features a young boy and a girl walk...</td>\n",
       "      <td>CC12M_HF/images/0001021/0005656.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7699715</th>\n",
       "      <td>https://www.wtravelmagazine.com/wp-content/upl...</td>\n",
       "      <td>ares station built purposefully within a large...</td>\n",
       "      <td>a group of people climbing through a cave at n...</td>\n",
       "      <td>The image captures a group of three people, tw...</td>\n",
       "      <td>ares station built purposefully within a large...</td>\n",
       "      <td>The image captures a moment of exploration in ...</td>\n",
       "      <td>Two people are walking in a cave with their li...</td>\n",
       "      <td>The image depicts a group of people exploring ...</td>\n",
       "      <td>CC12M_HF/images/0001021/0005657.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7699716</th>\n",
       "      <td>https://www.wspa.com/wp-content/uploads/sites/...</td>\n",
       "      <td>this week in history lewis and clark expeditio...</td>\n",
       "      <td>an advertisement for a book about the lewis an...</td>\n",
       "      <td>The image features an old-fashioned clock plac...</td>\n",
       "      <td>this week in history lewis and clark expeditio...</td>\n",
       "      <td>The image is a collage of two distinct element...</td>\n",
       "      <td>A clock is in the background of a poster adver...</td>\n",
       "      <td>The image features a large clock in the center...</td>\n",
       "      <td>CC12M_HF/images/0001021/0005658.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7699717 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Image Url   \n",
       "0        http://1.bp.blogspot.com/_dWmIOlGB7_0/S_HA9F1R...  \\\n",
       "1        http://2.bp.blogspot.com/-TwtZYTU4e9I/UQmEywkl...   \n",
       "2        http://2.bp.blogspot.com/--K8LdRsj99U/VQaOjr33...   \n",
       "3        http://2.bp.blogspot.com/-MBHbOy37tvU/TaX19Rrr...   \n",
       "4        http://2.bp.blogspot.com/-2HQcTDQ4oQs/V1oBoudL...   \n",
       "...                                                    ...   \n",
       "7699712  https://www.wtravelmagazine.com/wp-content/upl...   \n",
       "7699713  https://www.writeups.org/wp-content/uploads/Or...   \n",
       "7699714  https://www.wubbanub.com/wp-content/uploads/ca...   \n",
       "7699715  https://www.wtravelmagazine.com/wp-content/upl...   \n",
       "7699716  https://www.wspa.com/wp-content/uploads/sites/...   \n",
       "\n",
       "                                               raw_caption   \n",
       "0        this watch rolex gmt master has an independent...  \\\n",
       "1        picture of white modern bathroom furniture as ...   \n",
       "2        the american cowboy chronicles native american...   \n",
       "3        assimilation process in the indian style and f...   \n",
       "4        in mourning afterglow album lyrics in mourning...   \n",
       "...                                                    ...   \n",
       "7699712                        person on a yacht in cannes   \n",
       "7699713  orko he man and the masters of the universe ca...   \n",
       "7699714  old school ways to tell if youre having a boy ...   \n",
       "7699715  ares station built purposefully within a large...   \n",
       "7699716  this week in history lewis and clark expeditio...   \n",
       "\n",
       "                                          shortIB_captions   \n",
       "0                   a rolex watch with red and black dials  \\\n",
       "1        a white toilet sitting next to a sink in a bat...   \n",
       "2           a close up of a brown bear with its mouth open   \n",
       "3                  a woman in a pink sari sitting on a bed   \n",
       "4        a painting of a lighthouse in the middle of a ...   \n",
       "...                                                    ...   \n",
       "7699712    a man and woman standing on a boat in the water   \n",
       "7699713  two cartoon characters wearing hats and holdin...   \n",
       "7699714  a boy and a girl holding hands walking down th...   \n",
       "7699715  a group of people climbing through a cave at n...   \n",
       "7699716  an advertisement for a book about the lewis an...   \n",
       "\n",
       "                                           longIB_captions   \n",
       "0        The image showcases a stunning silver and red ...  \\\n",
       "1        The image showcases a modern and stylish bathr...   \n",
       "2        The image features a large brown bear with its...   \n",
       "3        In the picture, there are visible objects such...   \n",
       "4        The painting depicts a lighthouse standing on ...   \n",
       "...                                                    ...   \n",
       "7699712  In the image, there is a man and a woman posin...   \n",
       "7699713  In the picture, there are two cartoon characte...   \n",
       "7699714  The image captures two young children, a boy a...   \n",
       "7699715  The image captures a group of three people, tw...   \n",
       "7699716  The image features an old-fashioned clock plac...   \n",
       "\n",
       "                                          shortSV_captions   \n",
       "0        A Rolex watch with a black face and red and wh...  \\\n",
       "1        A bathroom with a white toilet, sink, and mirror.   \n",
       "2           A close up of a bear with its mouth wide open.   \n",
       "3           A woman in a long pink dress sitting on a bed.   \n",
       "4        A painting of a rocky island in the middle of ...   \n",
       "...                                                    ...   \n",
       "7699712                        person on a yacht in cannes   \n",
       "7699713  orko he man and the masters of the universe ca...   \n",
       "7699714  old school ways to tell if youre having a boy ...   \n",
       "7699715  ares station built purposefully within a large...   \n",
       "7699716  this week in history lewis and clark expeditio...   \n",
       "\n",
       "                                           longSV_captions   \n",
       "0        The image showcases a Rolex Submariner wristwa...  \\\n",
       "1        The image captures a modern bathroom, bathed i...   \n",
       "2        The image captures a close-up view of a brown ...   \n",
       "3        In the image, a woman is the central figure, s...   \n",
       "4        The image presents a dramatic scene of a rocky...   \n",
       "...                                                    ...   \n",
       "7699712  In the image, a man and a woman are standing o...   \n",
       "7699713  In the image, we see a scene from the animated...   \n",
       "7699714  In the image, two children are seen walking do...   \n",
       "7699715  The image captures a moment of exploration in ...   \n",
       "7699716  The image is a collage of two distinct element...   \n",
       "\n",
       "                                         shortLLA_captions   \n",
       "0         A silver and black watch with a red second hand.  \\\n",
       "1                       A bathroom with a toilet and sink.   \n",
       "2          A close up of a brown bear with its mouth open.   \n",
       "3           A woman is sitting on a bed with a pink dress.   \n",
       "4        A painting of a lighthouse on a rocky island s...   \n",
       "...                                                    ...   \n",
       "7699712          A man and a woman are standing on a boat.   \n",
       "7699713  Two cartoon characters in witch costumes are s...   \n",
       "7699714  A boy and a girl are walking down a sidewalk h...   \n",
       "7699715  Two people are walking in a cave with their li...   \n",
       "7699716  A clock is in the background of a poster adver...   \n",
       "\n",
       "                                          longLLA_captions   \n",
       "0        The image features a silver and black wristwat...  \\\n",
       "1        The image depicts a modern bathroom with a whi...   \n",
       "2        The image features a large brown bear with its...   \n",
       "3        The image features a beautiful woman wearing a...   \n",
       "4        The image depicts a dramatic scene of a lighth...   \n",
       "...                                                    ...   \n",
       "7699712  The image features a man and a woman standing ...   \n",
       "7699713  The image features two cartoon characters, bot...   \n",
       "7699714  The image features a young boy and a girl walk...   \n",
       "7699715  The image depicts a group of people exploring ...   \n",
       "7699716  The image features a large clock in the center...   \n",
       "\n",
       "                                  Image Path  \n",
       "0        CC12M_HF/images/0000000/0000000.jpg  \n",
       "1        CC12M_HF/images/0000000/0000002.jpg  \n",
       "2        CC12M_HF/images/0000000/0000003.jpg  \n",
       "3        CC12M_HF/images/0000000/0000004.jpg  \n",
       "4        CC12M_HF/images/0000000/0000005.jpg  \n",
       "...                                      ...  \n",
       "7699712  CC12M_HF/images/0001021/0005648.jpg  \n",
       "7699713  CC12M_HF/images/0001021/0005649.jpg  \n",
       "7699714  CC12M_HF/images/0001021/0005656.jpg  \n",
       "7699715  CC12M_HF/images/0001021/0005657.jpg  \n",
       "7699716  CC12M_HF/images/0001021/0005658.jpg  \n",
       "\n",
       "[7699717 rows x 9 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from data_config import DATADIR\n",
    "# csv = pd.read_csv('/home/mila/l/le.zhang/scratch/datasets/LAION/30M_laion_synthetic_filtered_large_with_path_filtered.csv')\n",
    "# csv = pd.read_csv(DATADIR['laion30m']['annotation'])\n",
    "csv = pd.read_csv(DATADIR['dreamclipcc12mhf']['annotation'])\n",
    "csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 compare text local embedding and online embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/l/le.zhang/.conda/envs/openflamingo/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/mila/l/le.zhang/.conda/envs/openflamingo/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NewModel(\n",
       "  (embeddings): NewEmbeddings(\n",
       "    (word_embeddings): Embedding(30528, 1024, padding_idx=0)\n",
       "    (rotary_emb): NTKScalingRotaryEmbedding()\n",
       "    (token_type_embeddings): Embedding(2, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): NewEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x NewLayer(\n",
       "        (attention): NewSdpaAttention(\n",
       "          (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): NewGatedMLP(\n",
       "          (up_gate_proj): Linear(in_features=1024, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act_fn): GELUActivation()\n",
       "          (hidden_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "        (mlp_ln): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "        (hidden_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_path = 'Alibaba-NLP/gte-large-en-v1.5'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "text_model = AutoModel.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.float16).to('cuda')\n",
    "text_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test index: [50824, 25799, 35055, 38208, 47015, 44581, 23830, 46923, 33966, 47656, 26871, 14638, 19949, 16046, 20038, 28226, 18838, 18273, 44978, 16335, 22804, 351, 23500, 39967, 40409, 25521, 33986, 51091, 17851, 21771, 37924, 38174, 18518, 39327, 43927, 24986, 27933, 15199, 14810, 11559, 40069, 48833, 15341, 21771, 41813, 25910, 28034, 23124, 9909, 51525, 12739, 40886, 35941, 12306, 10337, 28606, 37064, 49656, 46351, 13619, 31399, 49750, 47890, 14840, 46247, 47372, 42798, 30550, 15482, 35453, 44096, 49182, 18756, 30411, 11678, 17913, 22241, 51673, 31962, 47547, 38070, 44589, 15430, 37341, 30028, 19721, 14049, 12579, 32082, 23190, 40090, 22356, 23764, 50970, 24690, 41450, 28412, 33790, 33633, 15848, 44217, 47546, 26774, 19856, 20712, 19248, 25544, 48129, 31303, 31568, 39630, 25925, 42872, 36084, 18808, 25534, 22982, 12709, 37380, 24401, 24804, 51377, 27816, 42238, 46898, 48963, 29967, 14291, 16920, 42926, 19202, 47744, 41400, 22791, 22364, 16141, 12855, 36365, 37183, 9873, 26548, 24870, 19819, 37631, 18426, 50367, 51670, 48262, 24388, 47902, 29148, 130, 42059, 27317, 36271, 41638, 15132, 30037, 16791, 34049, 21589, 19399, 32341, 433, 50892, 29357, 45371, 11745, 25020, 37062, 16721, 49803, 46492, 33036, 24361, 27420, 45147, 12769, 13620, 14718, 44074, 14441, 40845, 22129, 20034, 39102, 35869, 33134, 44491, 37680, 29049, 9797, 31998, 19429, 30666, 43394, 49133, 49769, 14121, 23779, 17165, 40108, 25355, 39578, 38915, 19849, 47316, 34569, 20600, 38923, 26243, 33482, 49679, 12968, 23208, 19984, 42053, 48871, 48315, 11691, 34267, 20766, 28176, 39533, 20810, 23170, 22833, 31379, 31736, 21350, 36758, 45713, 44524, 23631, 36862, 34632, 17309, 17363, 31894, 12343, 12085, 33487, 33012, 49777, 39824, 41768, 37789, 27214, 13311, 34503, 22219, 45204, 24895, 23726, 24051, 22852, 32988, 13280, 36102, 38231, 35825, 37639, 44819, 34066, 15531, 45512, 49975, 50561, 11403, 25285, 389, 41606, 344, 13612, 14211, 31867, 44463, 35241, 38624, 48644, 37857, 45295, 35340, 25666, 15966, 14140, 12211, 24787, 30475, 12308, 28910, 21428, 37690, 43726, 37996, 12781, 32056, 41369, 389, 19063, 15742, 39323, 17889, 14950, 20320, 11958, 15718, 14061, 39977, 18038, 49602, 26161, 28582, 48748, 324, 32095, 17035, 17981, 35731, 34509, 14979, 36327, 35749, 38412, 30379, 49414, 25762, 15285, 432, 18923, 422, 26606, 27237, 43649, 37286, 38789, 51165, 22348, 51033, 12648, 40619, 38009, 15933, 48476, 46303, 22574, 50285, 45749, 20601, 28197, 29627, 24860, 38230, 44511, 19783, 36493, 43902, 22226, 38778, 47715, 19179, 43163, 25397, 22695, 20267, 42310, 48741, 50314, 22614, 44749, 36477, 33578, 43984, 44224, 41928, 30077, 35978, 9729, 34149, 29239, 44420, 31147, 31850, 16535, 21573, 43458, 37099, 16728, 41921, 10383, 36386, 35384, 33075, 15938, 38135, 18429, 33739, 17104, 14229, 33406, 12402, 30996, 11164, 39413, 37184, 49190, 31235, 25356, 38415, 16418, 25416, 45749, 38054, 27535, 37801, 36718, 10021, 10687, 18598, 47680, 15492, 28167, 24508, 46184, 14010, 35480, 19721, 23203, 49415, 38739, 42480, 47582, 39880, 34391, 233, 21722, 36632, 29235, 50197, 30494, 10113, 10577, 21693, 38207, 12332, 49906, 16715, 37102, 43168, 16578, 38412, 49, 20214, 31677, 17916, 35451, 31108, 46960, 24371, 36903, 19119, 25343, 17817, 27348, 39726, 23096, 47118, 41339, 13892, 41959, 32070, 18867, 34813, 24281, 34036, 10667, 35731, 51255, 45278, 36539, 11330, 39060, 20881, 40848, 401, 44916, 39376, 37283, 36711, 26184, 36621, 44765, 40600, 12986, 44852, 39275, 51266, 24837, 22478, 10410, 20925, 40417, 23194, 37317, 33741, 34261, 37092, 30616, 37011, 29103, 11672, 32727, 18657, 42134, 14936, 18691, 9877, 49175, 16096, 23963, 40295, 31003, 36165, 42222, 34493, 27985, 46964, 38349, 46290, 459, 44620, 20037, 51181, 13649, 34520, 48536, 43443, 22964, 12916, 11915, 220, 15919, 16122, 17279, 16561, 31958, 27587, 34144, 11323, 25856, 30476, 37006, 17513, 40579, 19363, 30418, 42613, 31269, 12694, 20874, 48286, 30462, 32593, 44049, 45242, 16766, 29332, 43532, 50258, 38386, 24276, 33081, 37815, 28375, 11565, 22991, 28839, 25721, 46016, 26398, 334, 45296, 22452, 35758, 46611, 22632, 42469, 44411, 31676, 29815, 51164, 19330, 27653, 21777, 15244, 16502, 17316, 34218, 33033, 22797, 9752, 47910, 35243, 24809, 18969, 48875, 18694, 17328, 45304, 49144, 23929, 10731, 37525, 47811, 23274, 35797, 27756, 28436, 27431, 19514, 30134, 50542, 42715, 17996, 14979, 46361, 45880, 17072, 50959, 14658, 13032, 32057, 24455, 51069, 18771, 48357, 36451, 17665, 19097, 21006, 11287, 28524, 10328, 30156, 19461, 13449, 27892, 10808, 35397, 25462, 50846, 25918, 21572, 26557, 33969, 30300, 35657, 15602, 10492, 37899, 34972, 43902, 30757, 46042, 39665, 35923, 12207, 17436, 48539, 46945, 28456, 41220, 15807, 40021, 21972, 24390, 17985, 39290, 22488, 46117, 17609, 41564, 13419, 25159, 11133, 17514, 36044, 13380, 11895, 42234, 11463, 35348, 45720, 44319, 15876, 33512, 42741, 41298, 34967, 22472, 40454, 24310, 22438, 20258, 43480, 42535, 44939, 19369, 49426, 28902, 30948, 35845, 27718, 43405, 28739, 11458, 25773, 44027, 10807, 35947, 45137, 49763, 39374, 17815, 44424, 28210, 46901, 40836, 49507, 22342, 26695, 45802, 45007, 45927, 33322, 29969, 11514, 37307, 32754, 29393, 80, 13314, 14092, 13039, 36130, 29598, 11471, 23675, 11409, 30629, 34209, 51464, 24050, 22428, 18096, 30277, 49836, 28770, 47545, 32805, 21961, 20798, 28348, 48796, 32414, 17426, 39620, 36214, 10918, 45919, 14049, 41565, 28488, 19157, 15724, 45030, 14752, 28801, 49750, 49140, 29530, 21547, 15520, 11010, 45582, 41828, 25477, 17303, 26624, 30546, 39140, 39759, 43813, 39934, 26333, 46051, 10255, 31450, 42653, 22027, 47079, 31056, 31627, 14293, 30421, 32753, 47430, 14209, 37422, 42355, 30430, 44518, 23746, 27679, 34519, 12386, 15112]\n",
      "Average difference over 1000 tests: 6.041526794433593e-07\n",
      "Max difference: 1.4901161193847656e-06\n",
      "Min difference: 4.172325134277344e-07\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Requires transformers>=4.36.0\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def compare_embeddings(n_tests=20, verbose=False):\n",
    "    def compute_difference(precomputed, computed):\n",
    "        return F.mse_loss(precomputed, computed).item()\n",
    "\n",
    "    differences = []\n",
    "    all_test_index = []\n",
    "    dimension = 512\n",
    "    for _ in range(n_tests):\n",
    "        file_idx = random.randint(0, 15038)\n",
    "        file_idx = random.randint(0, 100)\n",
    "        idx = random.randint(0, dimension-1)\n",
    "\n",
    "        # Load pre-computed embeddings\n",
    "        try:\n",
    "            x = torch.load(f'/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/text_embedding/gte-large-en-v1.5/dreamclipcc12mhf_shortSV_captions/{file_idx}.pt', weights_only=True)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Get the caption and its pre-computed embedding\n",
    "        caption = csv.iloc[file_idx*dimension+idx]['shortSV_captions']\n",
    "        precomputed_embedding = x[idx]\n",
    "\n",
    "        # Compute the embedding on-the-fly for comparison\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(caption, max_length=1024, padding=True, truncation=True, return_tensors='pt').to('cuda')\n",
    "            outputs = text_model(**inputs)\n",
    "            computed_embedding = outputs.last_hidden_state[:, 0].to(torch.float16)[0].cpu()\n",
    "\n",
    "        # Compute the difference\n",
    "        diff = compute_difference(precomputed_embedding, computed_embedding)\n",
    "        if diff>0.5:\n",
    "            print(f\"remove {file_idx}.pt, with diff {diff}\")\n",
    "            # remove\n",
    "            os.remove(f'/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/text_embedding/gte-large-en-v1.5/dreamclipcc12mhf_shortSV_captions/{file_idx}.pt')\n",
    "        differences.append(diff)\n",
    "        all_test_index.append(file_idx*dimension+idx)\n",
    "        if verbose:\n",
    "            print(f\"Test {_ + 1}:\")\n",
    "            print(f\"Caption: {caption} from index {file_idx*dimension+idx}\")\n",
    "            print(f\"Precomputed embedding : {precomputed_embedding}\")\n",
    "            print(f\"Computed embedding : {computed_embedding}\")\n",
    "            print(f\"Difference (MSE): {diff}\\n\")\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(f\"All test index: {all_test_index}\")\n",
    "    print(f\"Average difference over {n_tests} tests: {sum(differences) / n_tests}\")\n",
    "    print(f\"Max difference: {max(differences)}\")\n",
    "    print(f\"Min difference: {min(differences)}\")\n",
    "\n",
    "# Usage example:\n",
    "compare_embeddings(n_tests=1000, verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Test local image embedding and online computed embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dinov2Model(\n",
       "  (embeddings): Dinov2Embeddings(\n",
       "    (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "      (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): Dinov2Encoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x Dinov2Layer(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (attention): Dinov2SdpaAttention(\n",
       "          (attention): Dinov2SdpaSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): Dinov2SelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (layer_scale1): Dinov2LayerScale()\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Dinov2MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_scale2): Dinov2LayerScale()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "import os\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-large')\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-large',attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n",
    "# model = AutoModel.from_pretrained('facebook/dinov2-large')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "# model = torch.compile(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test indices: [758, 6989, 8491, 16173, 17461, 23372, 25892, 32297, 36039, 39138, 41149, 46913, 53235, 54677, 60544, 65380, 67525, 72672, 76540, 78548, 83515, 88005, 93093, 98146, 100380, 104508, 106756, 114029, 117111, 122433, 124935, 129129, 134129, 136543, 143182, 143660, 147480, 154354, 159021, 162603, 167728, 170706, 173056, 177564, 183608, 184972, 188936, 193194, 197755, 202909, 208269, 212327, 216781, 219585, 223301, 225692, 231168, 233950, 238622, 242849, 247468, 253450, 254373, 258695, 265637, 270327, 274335, 277548, 281753, 285543, 288175, 291804, 298724, 302602, 304492, 307337, 311954, 317898, 322158, 326072, 329540, 333895, 336904, 343121, 345375, 350332, 355900, 356968, 361502, 365375, 369597, 375128, 377914, 384876, 386960, 392357, 393303, 401057, 404012, 409352, 411668, 416957, 419407, 422562, 427804, 432909, 438219, 441406, 445331, 448507, 450943, 456582, 459755, 466002, 468486, 475071, 475252, 481481, 483746, 489823, 491606, 497783, 502102, 504202, 509822, 512273, 517664, 522438, 527105, 530036, 534556, 537214, 540825, 547453, 552372, 556935, 558041, 562134, 567122, 573271, 574871, 580834, 582500, 586253, 591378, 595803, 598326, 604427, 608035, 610590, 615427, 620348, 622747, 629524, 634521, 637443, 642215, 644297, 650890, 653593, 657939, 659918, 665196, 667654, 674350, 679230, 683433, 686548, 689797, 695171, 696560, 703755, 705303, 711367, 713421, 720301, 724148, 728285, 729733, 735839, 741104, 743008, 746096, 752225, 755045, 761729, 765228, 769047, 770267, 774850, 779399, 782801, 789682, 793391, 794896, 799001, 805622, 810233, 811076, 818421, 819790, 826112, 830293, 832395, 837254, 840553, 846154, 851101, 853699, 859493, 861416, 865364, 871456, 873634, 877031, 882757, 887984, 891659, 894860, 898991, 904942, 906274, 912544, 914450, 917627, 922750, 929225, 933281, 935688, 941204, 943783, 946585, 951191, 956072, 960671, 965977, 968521, 971154, 978183, 980272, 984382, 989962, 994682, 997340, 1002391, 1006850, 1011342, 1015344, 1018536, 1021895, 1027993, 1029703, 1034738, 1039958, 1042459, 1044752, 1049552, 1055426, 1060775, 1061469, 1067619, 1071488, 1074291, 1078714, 1083527, 1088211, 1093024, 1094685, 1097993, 1102542, 1108266, 1110825, 1115637, 1121481, 1123425, 1128882, 1132188, 1137954, 1139409, 1144390, 1149614, 1152533, 1158329, 1159520, 1165398, 1170815, 1175469, 1177677, 1179989, 1185351, 1189408, 1193153, 1196305, 1202899, 1205069, 1209206, 1215809, 1217590, 1224286, 1228752, 1232323, 1236253, 1237458, 1243472, 1248874, 1252081, 1257201, 1258231, 1263690, 1268836, 1271073, 1277134, 1278180, 1282974, 1286871, 1294272, 1294993, 1301029, 1305458, 1310323, 1314114, 1317854, 1322854, 1325556, 1328595, 1334233, 1335902, 1339778, 1346881, 1350287, 1355148, 1358763, 1361882, 1366387, 1371324, 1373773, 1376656, 1382674, 1384586, 1390197, 1394275, 1398189, 1403721, 1405832, 1409941, 1416586, 1420869, 1424215, 1427038, 1432656, 1433644, 1439033, 1442440, 1448445, 1450417, 1454189, 1461378, 1463630, 1467153, 1472518, 1474707, 1481552, 1483750, 1487390, 1493869, 1496564, 1501904, 1505068, 1507860, 1513251, 1518253, 1520310, 1525231, 1529371, 1533738, 1536147, 1541159, 1547257, 1551973, 1556140, 1558361, 1562640, 1564872, 1570070, 1576593, 1580614, 1583695, 1588826, 1592603, 1593630, 1600804, 1602086, 1607061, 1613142, 1616692, 1620970, 1625008, 1628697, 1630436, 1634484, 1640461, 1645811, 1647458, 1651321, 1658592, 1662524, 1663184, 1669868, 1673664, 1679300, 1680351, 1686295, 1687904, 1692927, 1696797, 1701587, 1707615, 1708921, 1714268, 1718679, 1721534, 1727735, 1730272, 1733383, 1740214, 1743844, 1748931, 1750161, 1753199, 1758615, 1761306, 1766306, 1770327, 1774852, 1780927, 1781793, 1788880, 1792456, 1797172, 1802002, 1804690, 1809546, 1814363, 1815580, 1821497, 1826726, 1827941, 1832191, 1838020, 1841430, 1846837, 1849042, 1851693, 1858924, 1860474, 1864317, 1870998, 1874085, 1877839, 1880348, 1885734, 1888864, 1894201, 1899827, 1903674, 1908051, 1912333, 1913856, 1920164, 1922171, 1928245, 1932224, 1936905, 1939738, 1941857, 1948901, 1949940, 1956952, 1960091, 1963877, 1966996, 1971735, 1976374, 1980061, 1983277, 1987958, 1991779, 1997062, 2001829, 2003966, 2011109, 2014596, 2016777, 2020405, 2024747, 2027986, 2032391, 2036920, 2043111, 2047946, 2049535, 2055830, 2057029, 2061305, 2066622, 2069983, 2074806, 2079531, 2084026, 2085686, 2091792, 2096326, 2099997, 2103762, 2107826, 2110654, 2116437, 2120627, 2124023, 2125865, 2131712, 2135953, 2138482, 2143206, 2150009, 2152116, 2156888, 2159302, 2163222, 2167826, 2172521, 2177786, 2181827, 2185438, 2188633, 2193592, 2199026, 2199779, 2207505, 2211069, 2212639, 2219846, 2223465, 2226878, 2231616, 2235703, 2239964, 2240586, 2246443, 2250668, 2255766, 2260036, 2263256, 2266743, 2272228, 2276138, 2281296, 2282972, 2286509, 2291232, 2293849, 2300281, 2304002, 2307525, 2313259, 2315844, 2321024, 2322926, 2328659, 2331666, 2337112, 2340751, 2344983, 2349096, 2355010, 2356433, 2362528, 2365092, 2368414, 2373212, 2379390, 2381209, 2385398, 2389596, 2394022, 2396687, 2404139, 2404922, 2408731, 2414740, 2417120, 2421308, 2426243, 2432729, 2436544, 2439050, 2442964, 2449362, 2452612, 2455002, 2459217, 2462900, 2469322, 2471866, 2477772, 2480859, 2485754, 2487696, 2493367, 2496769, 2499641, 2504633, 2508288, 2513332, 2518366, 2522944, 2523916, 2527732, 2531431, 2536017, 2540168, 2547077, 2549870, 2552958, 2559142, 2560264, 2566315, 2568781, 2573405, 2576452, 2582005, 2586422, 2590014, 2595258, 2599334, 2602655, 2606099, 2609936, 2614455, 2620810, 2624104, 2626577, 2633431, 2637095, 2640080, 2644041, 2649314, 2651807, 2655624, 2660163, 2664386, 2668346, 2672237, 2676074, 2679003, 2686846, 2688279, 2692717, 2695358, 2703001, 2705846, 2709085, 2712379, 2718115, 2722803, 2727434, 2729331, 2732549, 2739171, 2742542, 2745754, 2750117, 2752647, 2759287, 2761009, 2768646, 2772264, 2774097, 2779085, 2781427, 2786182, 2791990, 2796220, 2801146, 2803287, 2809383, 2813565, 2814816, 2818365, 2823649, 2828271, 2831766, 2834956, 2841893, 2842981, 2850781, 2854377, 2858250, 2861327, 2865435, 2870201, 2874246, 2878071, 2882065, 2884965, 2889116, 2894938, 2896710, 2902868, 2907310, 2909173, 2913731, 2919881, 2921414, 2925687, 2932009, 2933680, 2938476, 2944104, 2948749, 2949518, 2956727, 2957454, 2962652, 2966266, 2973485, 2974554, 2980852, 2984229, 2986386, 2994147, 2996823, 3000553, 3004236, 3008376, 3012591, 3018183, 3019079, 3026120, 3026990, 3034889, 3035969, 3041967, 3045276, 3048262, 3054303, 3058364, 3062373, 3066033, 3069863, 3073581, 3076773, 3083316, 3086213, 3089379, 3096341, 3099802, 3103601, 3105982, 3112280, 3113645, 3117259, 3123856, 3126053, 3129736, 3136281, 3140918, 3142516, 3146790, 3152736, 3154949, 3159528, 3163761, 3168707, 3173905, 3176755, 3181173, 3186597, 3189083, 3193643, 3198356, 3200340, 3205545, 3209339, 3215014, 3215639, 3222950, 3225665, 3228679, 3234175, 3237439, 3242820, 3245023, 3251004, 3254329, 3258149, 3262843, 3267203, 3268881, 3274988]\n",
      "Average difference over 1000 tests: 1.3301968574523925e-05\n",
      "Max difference: 0.0039520263671875\n",
      "Min difference: 1.2516975402832031e-06\n",
      "Time taken: 55.49853324890137 seconds\n"
     ]
    }
   ],
   "source": [
    "def compare_image_embeddings(model,processor,n_tests=5, verbose=False):\n",
    "    differences = []\n",
    "    all_test_indices = []\n",
    "    device = model.device\n",
    "\n",
    "    for idx in range(800):\n",
    "        # file_idx = random.randint(0, 5169)\n",
    "        file_idx = idx\n",
    "        idx = random.randint(0, 4096)  # Changed to 0-4095 to match the tensor size\n",
    "        test_index = file_idx * 4096 + idx\n",
    "        all_test_indices.append(test_index)\n",
    "\n",
    "        # image_path = os.path.join(DATADIR['laion30m']['imagedir'], csv.iloc[test_index]['Image Path'])\n",
    "        image_path = os.path.join(DATADIR['dreamclipcc12mhf']['imagedir'], csv.iloc[test_index]['Image Path'])\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Test {_ + 1}:\")\n",
    "            print(f\"Comparing image at index {test_index}\")\n",
    "\n",
    "        # Load pre-computed embedding\n",
    "        # emebdding_path = f\"/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-large/laion30m/{file_idx}.pt\"\n",
    "        emebdding_path = f\"/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-large/dreamclipcc12mhf/{file_idx}.pt\"\n",
    "        if not os.path.exists(emebdding_path):\n",
    "            continue\n",
    "        x = torch.load(emebdding_path, weights_only=True)\n",
    "        precomputed_embedding = x[idx].to(device)\n",
    "\n",
    "        # Compute embedding on-the-fly\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        sequence_output = outputs[0]\n",
    "        cls_token = sequence_output[:, 0]\n",
    "        patch_tokens = sequence_output[:, 1:]\n",
    "        computed_embedding = torch.cat([cls_token, patch_tokens.mean(dim=1)], dim=1).to(torch.float16)[0]\n",
    "\n",
    "        # Move embeddings back to CPU for comparison and storage\n",
    "        precomputed_embedding = precomputed_embedding.cpu()\n",
    "        computed_embedding = computed_embedding.cpu()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Precomputed embedding: {precomputed_embedding}\")\n",
    "            print(f\"Computed embedding: {computed_embedding}\")\n",
    "\n",
    "        # Compute the difference\n",
    "        diff = F.mse_loss(precomputed_embedding, computed_embedding).item()\n",
    "        differences.append(diff)\n",
    "        if diff>1:\n",
    "            # os.remove(emebdding_path)\n",
    "            print(f\"remove {emebdding_path}, with diff {diff}\")\n",
    "            \n",
    "        if verbose:\n",
    "            print(f\"Difference (MSE): {diff}\\n\")\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(f\"All test indices: {all_test_indices}\")\n",
    "    print(f\"Average difference over {n_tests} tests: {sum(differences) / n_tests}\")\n",
    "    print(f\"Max difference: {max(differences)}\")\n",
    "    print(f\"Min difference: {min(differences)}\")\n",
    "\n",
    "start_time = time.time()\n",
    "# Usage example:\n",
    "compare_image_embeddings(model,processor,n_tests=1000, verbose=False)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Manually inspect single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-large/laion30m/1272.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [111]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load pre-computed embedding\u001b[39;00m\n\u001b[1;32m     14\u001b[0m emebdding_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-large/laion30m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43memebdding_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m precomputed_embedding \u001b[38;5;241m=\u001b[39m x[idx]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Compute embedding on-the-fly\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/openflamingo/lib/python3.9/site-packages/torch/serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.conda/envs/openflamingo/lib/python3.9/site-packages/torch/serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.conda/envs/openflamingo/lib/python3.9/site-packages/torch/serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-large/laion30m/1272.pt'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "device = 'cuda'\n",
    "file_idx = random.randint(0, 5000)\n",
    "idx = random.randint(0, 4096)  # Changed to 0-4095 to match the tensor size\n",
    "test_index = file_idx * 4096 + idx\n",
    "\n",
    "image_path = os.path.join(DATADIR['laion30m']['imagedir'], csv.iloc[test_index]['Image Path'])\n",
    "image = Image.open(image_path)\n",
    "\n",
    "\n",
    "# Load pre-computed embedding\n",
    "emebdding_path = f\"/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-large/laion30m/{file_idx}.pt\"\n",
    "\n",
    "x = torch.load(emebdding_path, weights_only=True)\n",
    "precomputed_embedding = x[idx].to(device)\n",
    "\n",
    "# Compute embedding on-the-fly\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "sequence_output = outputs[0]\n",
    "cls_token = sequence_output[:, 0]\n",
    "patch_tokens = sequence_output[:, 1:]\n",
    "computed_embedding = torch.cat([cls_token, patch_tokens.mean(dim=1)], dim=1).to(torch.float16)[0]\n",
    "\n",
    "# Move embeddings back to CPU for comparison and storage\n",
    "precomputed_embedding = precomputed_embedding.cpu()\n",
    "computed_embedding = computed_embedding.cpu()\n",
    "print(f\"image index {test_index}\")\n",
    "print(f\"Precomputed embedding: {precomputed_embedding}\")\n",
    "print(f\"Computed embedding: {computed_embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Visualize the image and its caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "# Get the caption and image path\n",
    "idx = random.randint(0, len(df))\n",
    "caption = df.iloc[idx]['caption']\n",
    "image_path = os.path.join(\"/home/mila/l/le.zhang/scratch/datasets/LAION\", df.iloc[idx]['Image Path'])\n",
    "\n",
    "# Open and display the image\n",
    "img = Image.open(image_path)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "\n",
    "# Add the caption at the bottom of the image\n",
    "plt.text(0.5, -0.05, caption, wrap=True, horizontalalignment='center', \n",
    "         verticalalignment='top', transform=plt.gca().transAxes)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2501911/788722818.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x1 = torch.load('/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/text_embedding/gte-large-en-v1.5/validation/406.pt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 1024])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import random\n",
    "x1 = torch.load('/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/text_embedding/gte-large-en-v1.5/validation/406.pt')\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openflamingo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
