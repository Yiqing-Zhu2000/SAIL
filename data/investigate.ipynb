{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "\n",
    "save_names = ['llava_pretrain']\n",
    "data = {name: [] for name in save_names}\n",
    "file_path = '/home/mila/l/le.zhang/scratch/datasets/Cambrian-Alignment/jsons/alignment_2.5m.jsonl'\n",
    "base_path = '/home/mila/l/le.zhang/scratch/datasets/Cambrian-Alignment'\n",
    "\n",
    "# Count the total number of lines for the progress bar\n",
    "total_lines = sum(1 for _ in open(file_path))\n",
    "\n",
    "with jsonlines.open(file_path) as reader:\n",
    "    for entry in tqdm(reader, total=total_lines, desc=\"Processing entries\"):\n",
    "        data_split = entry['image'].split('/')[1]\n",
    "        \n",
    "        if data_split in save_names:\n",
    "            if data_split == 'llava_pretrain':\n",
    "                entry['image'] = entry['image'].replace('llava_pretrain/images', 'sbu558k')\n",
    "            image_path = os.path.join(base_path, entry['image'])\n",
    "            image_path = os.path.normpath(image_path)\n",
    "            \n",
    "            if os.path.exists(image_path):\n",
    "                data[data_split].append(entry)\n",
    "            else:\n",
    "                break\n",
    "# count the number of each dataset\n",
    "for key, entries in data.items():\n",
    "    print(f\"{key}: {len(entries)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/home/mila/l/le.zhang/scratch/datasets/Cambrian-Alignment/sbu558k/00120/001200924.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as json for each data\n",
    "for key, entries in data.items():\n",
    "    save_path = f'/home/mila/l/le.zhang/scratch/datasets/Cambrian-Alignment/jsons/{key}.json'\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(entries, f)\n",
    "    print(f\"Saved {len(entries)} entries to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "def get_linear_input(sequence_output):\n",
    "    cls_token = sequence_output[:, 0]\n",
    "    patch_tokens = sequence_output[:, 1:]\n",
    "    linear_input = torch.cat([cls_token, patch_tokens.mean(dim=1)], dim=1)\n",
    "    return linear_input\n",
    "directory = '/home/mila/l/le.zhang/scratch/light_align/data/image_embedding/dinov2-base'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".pt\"):\n",
    "        print(os.path.join(directory, filename))\n",
    "        image_embedding = torch.load(os.path.join(directory, filename))\n",
    "        image_embedding = get_linear_input(image_embedding)\n",
    "        torch.save(image_embedding, os.path.join(directory, filename))\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "def compare_random_tensor_value(pt_file1, pt_file2, position=None):\n",
    "    # 加载 .pt 文件\n",
    "    tensor1 = torch.load(pt_file1,weights_only=True)\n",
    "    tensor2 = torch.load(pt_file2,weights_only=True)\n",
    "    # 如果没有指定位置，随机选择一个索引位置\n",
    "    if position is None:\n",
    "        position = random.randint(0, len(tensor1) - 1) \n",
    "    # 获取指定位置的值\n",
    "    value1 = tensor1[position]\n",
    "    value2 = tensor2[position]\n",
    "    \n",
    "    # 比较两个值并打印结果\n",
    " \n",
    "    print(f\"The values at position {position} of index {index} are different.\\n\"\n",
    "              f\"File 1 value: {value1}\\nFile 2 value: {value2}\")\n",
    "    \n",
    "    # return value1, value2\n",
    "\n",
    "index = random.randint(150,200)\n",
    "pt_file1 = f\"/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/text_embedding/gte-large-en-v1.5/dreamclipcc3m_raw_caption/{index}.pt\"\n",
    "pt_file2 = f\"/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/text_embedding/gte-large-en-v1.5/dreamclipcc3m_raw/{index}.pt\"\n",
    "compare_random_tensor_value(pt_file1,pt_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.randint(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from natsort import natsorted\n",
    "path_list = ['/home/mila/l/le.zhang/scratch/light_align/data/image_embedding/dinov2-large/ALLaVAVFLAN', '/home/mila/l/le.zhang/scratch/light_align/data/image_embedding/dinov2-large/LLaVA558K']\n",
    "all_files = []\n",
    "for dir_path in path_list:\n",
    "    files = glob.glob(os.path.join(dir_path, \"*.pt\"))\n",
    "    sorted_files = natsorted(files)\n",
    "    all_files.extend(sorted_files)\n",
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "# read json\n",
    "with open('/home/mila/l/le.zhang/scratch/datasets/Cambrian-Alignment/jsons/llava_pretrain.json', 'r') as f:\n",
    "    sharegpt4v = json.load(f)\n",
    "with open('/home/mila/l/le.zhang/scratch/datasets/Cambrian-Alignment/jsons/sbu558k.json', 'r') as f:\n",
    "    sbu558k = json.load(f)\n",
    "with open('/home/mila/l/le.zhang/scratch/datasets/Cambrian-Alignment/jsons/coco.json', 'r') as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to caculate avg length \n",
    "def avg_length(data):\n",
    "    avg = 0\n",
    "    for entry in data:\n",
    "        avg += len(entry['conversations'][-1]['value'])\n",
    "    return avg/len(data)\n",
    "\n",
    "avg_length(sharegpt4v), avg_length(sbu558k), avg_length(coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharegpt4v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from natsort import natsorted\n",
    "import glob\n",
    "\n",
    "def merge_pt_files(directory):\n",
    "    # 获取所有的 .pt 文件路径，并按名字顺序排序\n",
    "    files = glob.glob(os.path.join(directory, \"*.pt\"))\n",
    "    files = natsorted(files)\n",
    "    \n",
    "    # 确保文件数量为偶数\n",
    "    if len(files) % 2 != 0:\n",
    "        raise ValueError(\"The number of .pt files is not even. Please make sure the files can be paired.\")\n",
    "\n",
    "    # 遍历所有文件，按两个为一组进行合并\n",
    "    for i in range(0, len(files), 2):\n",
    "        file1 = files[i]\n",
    "        file2 = files[i+1]\n",
    "        \n",
    "        # 加载两个文件\n",
    "        tensor1 = torch.load(file1)\n",
    "        tensor2 = torch.load(file2)\n",
    "        \n",
    "        # 在第一个维度进行拼接\n",
    "        merged_tensor = torch.cat((tensor1, tensor2), dim=0)\n",
    "        \n",
    "        # 生成新文件名\n",
    "        base_name1 = os.path.basename(file1).replace(\".pt\", \"\")\n",
    "        base_name2 = os.path.basename(file2).replace(\".pt\", \"\")\n",
    "        new_file_name = f\"{base_name1}_{base_name2}.pt\"\n",
    "        new_file_path = os.path.join(directory, new_file_name)\n",
    "        \n",
    "        # 保存拼接后的tensor\n",
    "        torch.save(merged_tensor, new_file_path)\n",
    "        print(f\"Saved merged tensor to {new_file_path}\")\n",
    "\n",
    "# 示例用法\n",
    "# merge_pt_files('/path/to/your/directory')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "file_dir = \"/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/text_embedding/gte-large-en-v1.5/laion30m_caption\"\n",
    "files = glob.glob(os.path.join(file_dir, \"*.pt\"))\n",
    "total_len = 0\n",
    "for file in tqdm(files):\n",
    "    tensor = torch.load(file, map_location=torch.device('cpu'), weights_only=False)\n",
    "    total_len += tensor.size(0)\n",
    "    if tensor.size(0) != 4096:\n",
    "        print(file, tensor.size(0))\n",
    "print(total_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 2707/5170 [01:09<01:02, 39.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-large/laion30m/5169.pt 642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5170/5170 [02:17<00:00, 37.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21172866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "file_dir = \"/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-large/laion30m\"\n",
    "files = glob.glob(os.path.join(file_dir, \"*.pt\"))\n",
    "total_len = 0\n",
    "for file in tqdm(files):\n",
    "    tensor = torch.load(file, map_location=torch.device('cpu'), weights_only=False)\n",
    "    total_len += tensor.size(0)\n",
    "    if tensor.size(0) != 4096:\n",
    "        print(file, tensor.size(0))\n",
    "print(total_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "vision_embedding = torch.load('/home/mila/l/le.zhang/scratch/light_align/data/text_embedding/gte-large-en-v1.5/dreamclipcc12m_longSV_captions/5859.pt')\n",
    "print(vision_embedding.shape)   \n",
    "vision_embedding[54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "file_dir = \"/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-base/dreamclipcc12mhf\"\n",
    "files = glob.glob(os.path.join(file_dir, \"*.pt\"))\n",
    "total_len = 0\n",
    "for file in tqdm(files):\n",
    "    tensor = torch.load(file, map_location=torch.device('cpu'), weights_only=False)\n",
    "    total_len += tensor.size(0)\n",
    "    if tensor.size(0) != 1024:\n",
    "        print(file, tensor.size(0))\n",
    "print(total_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "file_dir = \"/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-giant/dreamclipcc3m\"\n",
    "files = glob.glob(os.path.join(file_dir, \"*.pt\"))\n",
    "total_len = 0\n",
    "for file in tqdm(files):\n",
    "    tensor = torch.load(file, map_location=torch.device('cpu'), weights_only=False)\n",
    "    total_len += tensor.size(0)\n",
    "    if tensor.size(0) != 1024:\n",
    "        print(file, tensor.size(0))\n",
    "print(total_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_config import DATADIR\n",
    "# load csv file\n",
    "csv = pd.read_csv(DATADIR['dreamclipcc12mhf']['annotation'], nrows=100000)\n",
    "csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open image and show\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "# 指定index为5的行是我们要显示的图像\n",
    "index = random.randint(0, 100000)\n",
    "image_path = os.path.join(DATADIR['dreamclipcc12mhf']['imagedir'], csv.iloc[index]['Image Path'])\n",
    "\n",
    "# 打开图像并显示 标题为caption\n",
    "image = Image.open(image_path)\n",
    "plt.imshow(image)\n",
    "plt.title(\"\\n\".join(csv.iloc[index]['longLLA_captions'].split(\" \")), loc='left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: test if embedding are encoded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption</th>\n",
       "      <th>url</th>\n",
       "      <th>Image Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the tiki ball logo is displayed on this apron</td>\n",
       "      <td>https://image.spreadshirtmedia.com/image-serve...</td>\n",
       "      <td>LAION30M/images/0000000/0000000.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the view of an aerial pool with palm trees</td>\n",
       "      <td>http://uberflip.cdntwrk.com/mediaproxy?url=htt...</td>\n",
       "      <td>LAION30M/images/0000000/0000001.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a sky with clouds women's v - neck</td>\n",
       "      <td>https://render.fineartamerica.com/images/rende...</td>\n",
       "      <td>LAION30M/images/0000000/0000002.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the brain coffee mug</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/2438/3835/pr...</td>\n",
       "      <td>LAION30M/images/0000000/0000003.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the blue sedan is parked on a street</td>\n",
       "      <td>https://http2.mlstatic.com/ford-focus-20-st-5-...</td>\n",
       "      <td>LAION30M/images/0000000/0000006.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21172861</th>\n",
       "      <td>the peace revolution episode 9 of reclaimivism</td>\n",
       "      <td>http://4.bp.blogspot.com/-q_984YJqDOw/TnDihomT...</td>\n",
       "      <td>LAION30M/images/0002999/0009995.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21172862</th>\n",
       "      <td>2 pack - body - multi</td>\n",
       "      <td>https://img01.ztat.net/article/spp-media-p1/f3...</td>\n",
       "      <td>LAION30M/images/0002999/0009996.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21172863</th>\n",
       "      <td>a large crystal chandelier hanging from the ce...</td>\n",
       "      <td>http://img1.wfrcdn.com/lf/50/hash/2023/866064/...</td>\n",
       "      <td>LAION30M/images/0002999/0009997.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21172864</th>\n",
       "      <td>a white bed with white pillows</td>\n",
       "      <td>https://akamai-scene7.ballarddesigns.com/is/im...</td>\n",
       "      <td>LAION30M/images/0002999/0009998.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21172865</th>\n",
       "      <td>the iphone5 in white and black</td>\n",
       "      <td>https://img.diytrade.com/smimg/951746/24087526...</td>\n",
       "      <td>LAION30M/images/0002999/0009999.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21172866 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    caption   \n",
       "0             the tiki ball logo is displayed on this apron  \\\n",
       "1                the view of an aerial pool with palm trees   \n",
       "2                        a sky with clouds women's v - neck   \n",
       "3                                      the brain coffee mug   \n",
       "4                      the blue sedan is parked on a street   \n",
       "...                                                     ...   \n",
       "21172861     the peace revolution episode 9 of reclaimivism   \n",
       "21172862                              2 pack - body - multi   \n",
       "21172863  a large crystal chandelier hanging from the ce...   \n",
       "21172864                     a white bed with white pillows   \n",
       "21172865                     the iphone5 in white and black   \n",
       "\n",
       "                                                        url   \n",
       "0         https://image.spreadshirtmedia.com/image-serve...  \\\n",
       "1         http://uberflip.cdntwrk.com/mediaproxy?url=htt...   \n",
       "2         https://render.fineartamerica.com/images/rende...   \n",
       "3         https://cdn.shopify.com/s/files/1/2438/3835/pr...   \n",
       "4         https://http2.mlstatic.com/ford-focus-20-st-5-...   \n",
       "...                                                     ...   \n",
       "21172861  http://4.bp.blogspot.com/-q_984YJqDOw/TnDihomT...   \n",
       "21172862  https://img01.ztat.net/article/spp-media-p1/f3...   \n",
       "21172863  http://img1.wfrcdn.com/lf/50/hash/2023/866064/...   \n",
       "21172864  https://akamai-scene7.ballarddesigns.com/is/im...   \n",
       "21172865  https://img.diytrade.com/smimg/951746/24087526...   \n",
       "\n",
       "                                   Image Path  \n",
       "0         LAION30M/images/0000000/0000000.jpg  \n",
       "1         LAION30M/images/0000000/0000001.jpg  \n",
       "2         LAION30M/images/0000000/0000002.jpg  \n",
       "3         LAION30M/images/0000000/0000003.jpg  \n",
       "4         LAION30M/images/0000000/0000006.jpg  \n",
       "...                                       ...  \n",
       "21172861  LAION30M/images/0002999/0009995.jpg  \n",
       "21172862  LAION30M/images/0002999/0009996.jpg  \n",
       "21172863  LAION30M/images/0002999/0009997.jpg  \n",
       "21172864  LAION30M/images/0002999/0009998.jpg  \n",
       "21172865  LAION30M/images/0002999/0009999.jpg  \n",
       "\n",
       "[21172866 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from data_config import DATADIR\n",
    "# csv = pd.read_csv('/home/mila/l/le.zhang/scratch/datasets/LAION/30M_laion_synthetic_filtered_large_with_path_filtered.csv')\n",
    "csv = pd.read_csv(DATADIR['laion30m']['annotation'])\n",
    "csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 compare text local embedding and online embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewModel(\n",
       "  (embeddings): NewEmbeddings(\n",
       "    (word_embeddings): Embedding(30528, 1024, padding_idx=0)\n",
       "    (rotary_emb): NTKScalingRotaryEmbedding()\n",
       "    (token_type_embeddings): Embedding(2, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): NewEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x NewLayer(\n",
       "        (attention): NewSdpaAttention(\n",
       "          (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): NewGatedMLP(\n",
       "          (up_gate_proj): Linear(in_features=1024, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act_fn): GELUActivation()\n",
       "          (hidden_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "        (mlp_ln): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "        (hidden_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_path = 'Alibaba-NLP/gte-large-en-v1.5'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "text_model = AutoModel.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.float16).to('cuda')\n",
    "text_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test index: [11224793, 2314150, 17755481, 7629665, 12830764, 3611940, 19842386, 9227036, 9298819, 17344, 1941918, 9673912, 6833064, 17001136, 323185, 4582732, 7825298, 18901270, 6991378, 15200084, 16075713, 14582865, 16122920, 15649797, 1311608, 7053571, 18385141, 16339891, 1208540, 12412815, 14391713, 1373392, 13999264, 16903562, 9304337, 16575195, 994195, 13436781, 5186432, 12590612, 17034015, 12176162, 5945395, 17714559, 20422945, 15467545, 289533, 11976424, 12602272, 9449434, 13833732, 13190228, 15619484, 18901216, 16840990, 4628505, 2685743, 2439412, 6117084, 10234059, 13595409, 8452952, 216992, 5282152, 2945796, 265070, 18034370, 1220369, 15331761, 1369652, 11632682, 11518093, 11024603, 1401945, 13261722, 14345905, 20382069, 14018851, 2795945, 1632947, 12142869, 5037332, 17264291, 18749304, 1902694, 14062173, 15890297, 18762979, 10690189, 16205159, 1607592, 20104139, 4011394, 12588542, 11019574, 8994017, 9634936, 8913977, 18426912, 17572725, 5906248, 3244706, 7946656, 10806034, 15399661, 10006690, 2355053, 4448513, 18777007, 14832414, 6149036, 4478422, 14728673, 7758137, 20348314, 3104070, 18501901, 11709360, 4180436, 19304232, 12000709, 7432350, 2294845, 1100452, 77938, 2112338, 1141882, 4743659, 2016495, 13465627, 5930888, 12886465, 19946782, 14004699, 6585400, 16890116, 16800041, 3817334, 19927525, 10213099, 7570023, 4931995, 16619248, 17681825, 615948, 3195142, 10133841, 3798189, 13431374, 15549232, 575848, 7587920, 14441586, 10467323, 17408698, 12426457, 2867577, 1333423, 3702702, 19204043, 7037795, 1547559, 13923663, 12685782, 3653399, 8636101, 17412223, 2940712, 2312487, 7939650, 9825149, 8000651, 1167022, 10491290, 2439264, 9012660, 7424485, 2632036, 9793737, 8706129, 4815434, 5500724, 20022660, 7673081, 8737037, 11734528, 3695039, 15108820, 17247958, 13776059, 12233892, 682603, 14317344, 5648313, 15060972, 2598554, 16843777, 12339353, 8496552, 415740, 17083872, 13246734, 4232388, 16819109, 319952, 12463651, 6704264, 5239203, 18963651, 5529624, 2462063, 2855200, 666537, 18638, 3317117, 7679361, 19582927, 5690477, 9296207, 9078676, 942764, 19212532, 6290282, 16052586, 11633043, 11012058, 8161521, 4339418, 1067990, 9624549, 14925369, 14367737, 11838649, 6184477, 11401400, 4518993, 17708122, 10461448, 445556, 504163, 5866577, 9784534, 70586, 1448536, 6743111, 5762837, 4392235, 14742476, 1587366, 2674298, 20006815, 15092944, 18968500, 5329967, 18714233, 195032, 16130471, 16206321, 5749150, 7908244, 13758929, 18413372, 12963061, 6718882, 15157376, 991556, 2810514, 9678885, 17394608, 14294882, 14803563, 14387070, 1134144, 20134470, 4618691, 13948900, 19044969, 6540981, 15106762, 15754934, 9987674, 290589, 2085377, 5680823, 382754, 19554662, 7718190, 16293993, 16143477, 14385233, 9673117, 12772082, 10426201, 6556527, 14164614, 15045909, 4003018, 14713262, 3624490, 8732083, 9907291, 20201853, 6728642, 17426479, 9666818, 6174438, 18626017, 7116544, 2049404, 1481278, 20251430, 2578674, 7732867, 20043633, 5633261, 6256808, 13046783, 2171820, 8655573, 7774613, 2117047, 12325576, 5390188, 9217334, 8890932, 3333533, 13888039, 12243205, 16857234, 19898657, 13203626, 691254, 5682007, 14851122, 2776899, 17133761, 6827638, 14436387, 16910971, 20073290, 19601755, 5683354, 17494965, 17789244, 15862609, 8820741, 7532714, 6456661, 17385871, 19158079, 8379762, 9570241, 17878374, 18120613, 12748519, 16501484, 11591428, 19116172, 14523121, 12628997, 6773645, 10561571, 19989600, 20354114, 10683065, 5022380, 10427952, 8733210, 4949725, 17182214, 13241460, 13180902, 11141189, 6353800, 6193005, 6099449, 12764346, 5649760, 13568578, 19124685, 18733079, 19800525, 9424288, 6742270, 2708126, 3038517, 12741393, 13197508, 2043518, 750882, 15876746, 3478995, 13713762, 17441106, 17924910, 1846286, 7090561, 6904148, 18697538, 20280645, 12183294, 20206117, 16007534, 5591874, 3993619, 18218502, 9124198, 2472811, 5117912, 2502044, 6908426, 4931290, 3756795, 15737028, 2401721, 1811826, 11812302, 18103327, 11141493, 1994935, 2422554, 1420175, 9446358, 5947404, 13406931, 13086253, 15956185, 17366863, 3690015, 8809462, 20352139, 10660862, 7071829, 1008482, 3290068, 19760377, 19543737, 12837464, 10240868, 19860299, 6388359, 1278389, 14426068, 17250147, 12202751, 15725017, 12126611, 20116578, 15829257, 1876858, 6596323, 20387320, 14125648, 8822349, 3677432, 1174371, 2534549, 2894548, 12600472, 8125078, 9112656, 17185223, 12053464, 18970166, 4583536, 20298655, 10084772, 5600479, 1461492, 13166645, 15611372, 7772871, 1577720, 1775497, 7516819, 12186305, 14872651, 17028474, 816344, 2139860, 9163647, 9255348, 8396862, 3771395, 10137645, 19615365, 13518999, 13190309, 10167377, 14962155, 7633927, 18272196, 15949338, 171994, 5935320, 5269545, 5496709, 5539831, 9100999, 13455759, 4086043, 2901093, 3432308, 12322395, 16818177, 6739307, 12601455, 9229668, 1975817, 12927942, 8549125, 15634530, 14694736, 16929607, 13486399, 8790268, 8617996, 10524812, 15508462, 19789321, 7697122, 13134465, 8185517, 4716132, 18283496, 14724160, 9492786, 19580091, 14935178, 13955144, 3823364, 7198734, 16710476, 15183096, 5684711, 17333821, 3068219, 4433390, 17651345, 3695962, 16472781, 19923540, 14320249, 15549071, 18456797, 13754637, 8194906, 7115148, 8148363, 12485419, 15022697, 12770160, 13521483, 2205374, 9525264, 19463825, 469276, 15638636, 11218075, 6418076, 17057435, 13149024, 734943, 18366662, 5089291, 13306842, 5362815, 12062271, 8765357, 9073951, 15226608, 18000379, 11514515, 12650802, 16724269, 13650847, 1227034, 15744205, 9435296, 188364, 7931520, 302633, 10084494, 3678768, 14802996, 6869187, 9516404, 12736522, 4707230, 18098544, 15294511, 4552041, 1356163, 6788009, 10583297, 12615219, 12518468, 1924146, 18890612, 1503789, 5190908, 12219239, 17408384, 7751913, 15546631, 1176484, 2301702, 4837781, 3683424, 8353676, 13970456, 2314769, 14586038, 12632163, 10420980, 10365636, 13664034, 3213727, 2401584, 1557844, 9936269, 3545658, 6642234, 10732093, 13438216, 10081208, 447896, 19130711, 12016531, 10537330, 4207979, 15260013, 14923382, 8078743, 16846737, 18329658, 2116889, 18460960, 11573536, 5671500, 19651468, 11938871, 20130559, 19340356, 10266455, 6323859, 6705392, 2672449, 1286689, 17963004, 681858, 9337330, 12226794, 17793976, 746637, 32364, 17588554, 10084814, 11999588, 8065341, 10758537, 2106434, 4838530, 6996303, 7650340, 12556072, 19268314, 10962876, 11430154, 2239147, 6574138, 1904670, 1952386, 19164272, 17362836, 8408915, 10575769, 14826614, 2990920, 3392196, 4869730, 16416119, 4542578, 19865662, 9454263, 8334308, 12091574, 15746474, 3851468, 17475500, 14856223, 5227796, 1440860, 7929945, 19569540, 59458, 395304, 5549293, 6707921, 17297029, 13110189, 19589439, 6166015, 5481332, 4296786, 2821213, 13550085, 9377881, 8805711, 3853597, 2004265, 8545029, 12268001, 8855762, 9674407, 10313449, 8487206, 19960475, 5164279, 16163454, 11738705, 13752453, 16995450, 6531554, 15563479, 10193442, 19232908, 10162868, 7458590, 20409959, 2537248, 4621914, 265959, 18631941, 8796802, 17865477, 19761980, 9622033, 14480554, 1570553, 18517352, 8218784, 954928, 17514055, 12016843, 10740430, 15279888, 18108641, 7738797, 2906232, 13421117, 16682779, 10667859, 12787222, 15919675, 5403323, 4979610, 19022866, 8975331, 6532744, 17307314, 9442296, 16038816, 4326349, 18528016, 12260868, 6104973, 6770116, 6428275, 3381473, 5942750, 6195319, 9616839, 6890499, 612080, 11330407, 6037349, 12546765, 11667835, 11212846, 12611883, 12860206, 14573320, 12495043, 7836102, 1409763, 20049473, 13873315, 12727724, 13364160, 3405470, 13695715, 17318377, 4685259, 17016449, 14133986, 14453015, 3541422, 12027955, 18721818, 11610222, 4278984, 10536914, 17369318, 206174, 5403136, 5931926, 132421, 12145288, 17797719, 1362350, 9910630, 9589223, 14005736, 12991496, 5016588, 17253638, 10006822, 5986773, 3889093, 19825430, 17718671, 1137570, 16559236, 14852822, 9337415, 1243791, 13683181, 11609194, 12404823, 11526300, 16918256, 12316508, 15898638, 3778693, 16267249, 3822557, 14649279, 12060955, 3922860, 26112, 8469911, 14731431, 18085043, 20346530, 285635, 9092423, 2923063, 17230787, 245543, 13187541, 1411357, 12880350, 9615589, 8970601, 6013927, 7371116, 17612188, 7971173, 18493549, 14542516, 16561510, 973826, 6879636, 15315829, 12827707, 17779823, 16986432, 2686514, 17167373, 17133262, 1066575, 16347119, 18965042, 1261376, 2976995, 14960675, 12170907, 10927635, 8431563, 18979934, 5809627, 13987721, 1136217, 15552341, 6812435, 411317, 14552885, 9897143, 17187885, 18752971, 2500515, 17751424, 12105953, 749631, 14277958, 444233, 1171701, 18853641, 2115425, 16663800, 10391977, 18046382, 6401562, 10532928, 14253412, 17250646, 993928, 17830293, 16630626, 16268080, 3027327, 10277639, 10006160, 2728372, 4048982, 19509284, 2456106, 3665969, 4383094, 8059719, 15478115, 1769134, 19948310, 7787224, 11925572, 12726653, 975160, 18045131, 12272783, 16125571, 18257185, 1710969, 18520136, 14336313, 14737427, 19778451, 4358155, 535613, 17804545, 7480102, 5779277, 18768756, 18571307, 20033371, 5789092, 17237995, 3960028, 15159630, 13537613, 20271804, 16827103, 15561254, 13087834, 5901097, 15096752, 19502903, 7953157, 9078220, 2971419, 17391606, 276354, 10268608, 4804904, 995271, 4184074, 14220369, 13260568, 18501970, 5013877, 10378989, 9164568, 8234198, 5475851, 15109324, 12556140, 5750280, 7363716, 10599158, 2686655, 4881299, 17674093, 20145080, 16247125, 4242801, 18994646, 14645094, 18222688, 3159172, 16257495, 9090764, 10096562, 12472500, 14663970, 10074863, 5490297, 12317829, 8375405, 9063570, 4994506]\n",
      "Average difference over 1000 tests: 7.715225219726563e-07\n",
      "Max difference: 3.2782554626464844e-06\n",
      "Min difference: 4.172325134277344e-07\n"
     ]
    }
   ],
   "source": [
    "# Requires transformers>=4.36.0\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def compare_embeddings(n_tests=20, verbose=False):\n",
    "    def compute_difference(precomputed, computed):\n",
    "        return F.mse_loss(precomputed, computed).item()\n",
    "\n",
    "    differences = []\n",
    "    all_test_index = []\n",
    "    for _ in range(n_tests):\n",
    "        file_idx = random.randint(0, 5000)\n",
    "        idx = random.randint(0, 4096)\n",
    "\n",
    "        # Load pre-computed embeddings\n",
    "        x = torch.load(f'/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/text_embedding/gte-large-en-v1.5/3mvalid/{file_idx}.pt', weights_only=True)\n",
    "\n",
    "        # Get the caption and its pre-computed embedding\n",
    "        caption = csv.iloc[file_idx*4096+idx]['caption']\n",
    "        precomputed_embedding = x[idx]\n",
    "\n",
    "        # Compute the embedding on-the-fly for comparison\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(caption, max_length=8192, padding=True, truncation=True, return_tensors='pt').to('cuda')\n",
    "            outputs = text_model(**inputs)\n",
    "            computed_embedding = outputs.last_hidden_state[:, 0].to(torch.float16)[0].cpu()\n",
    "\n",
    "        # Compute the difference\n",
    "        diff = compute_difference(precomputed_embedding, computed_embedding)\n",
    "        differences.append(diff)\n",
    "        all_test_index.append(file_idx*4096+idx)\n",
    "        if verbose:\n",
    "            print(f\"Test {_ + 1}:\")\n",
    "            print(f\"Caption: {caption} from index {file_idx*4096+idx}\")\n",
    "            print(f\"Precomputed embedding : {precomputed_embedding}\")\n",
    "            print(f\"Computed embedding : {computed_embedding}\")\n",
    "            print(f\"Difference (MSE): {diff}\\n\")\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(f\"All test index: {all_test_index}\")\n",
    "    print(f\"Average difference over {n_tests} tests: {sum(differences) / n_tests}\")\n",
    "    print(f\"Max difference: {max(differences)}\")\n",
    "    print(f\"Min difference: {min(differences)}\")\n",
    "\n",
    "# Usage example:\n",
    "compare_embeddings(n_tests=1000, verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Test local image embedding and online computed embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dinov2Model(\n",
       "  (embeddings): Dinov2Embeddings(\n",
       "    (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "      (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): Dinov2Encoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x Dinov2Layer(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (attention): Dinov2SdpaAttention(\n",
       "          (attention): Dinov2SdpaSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): Dinov2SelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (layer_scale1): Dinov2LayerScale()\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Dinov2MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_scale2): Dinov2LayerScale()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "import os\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-large')\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-large',attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n",
    "# model = AutoModel.from_pretrained('facebook/dinov2-large')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "# model = torch.compile(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/l/le.zhang/.conda/envs/openflamingo/lib/python3.9/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def compare_image_embeddings(model,processor,n_tests=5, verbose=False):\n",
    "    differences = []\n",
    "    all_test_indices = []\n",
    "    device = model.device\n",
    "\n",
    "    for idx in range(5169):\n",
    "        # file_idx = random.randint(0, 5169)\n",
    "        file_idx = idx\n",
    "        idx = random.randint(0, 4096)  # Changed to 0-4095 to match the tensor size\n",
    "        test_index = file_idx * 4096 + idx\n",
    "        all_test_indices.append(test_index)\n",
    "\n",
    "        image_path = os.path.join(DATADIR['laion30m']['imagedir'], csv.iloc[test_index]['Image Path'])\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Test {_ + 1}:\")\n",
    "            print(f\"Comparing image at index {test_index}\")\n",
    "\n",
    "        # Load pre-computed embedding\n",
    "        emebdding_path = f\"/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-large/laion30m/{file_idx}.pt\"\n",
    "        if not os.path.exists(emebdding_path):\n",
    "            continue\n",
    "        x = torch.load(emebdding_path, weights_only=True)\n",
    "        precomputed_embedding = x[idx].to(device)\n",
    "\n",
    "        # Compute embedding on-the-fly\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        sequence_output = outputs[0]\n",
    "        cls_token = sequence_output[:, 0]\n",
    "        patch_tokens = sequence_output[:, 1:]\n",
    "        computed_embedding = torch.cat([cls_token, patch_tokens.mean(dim=1)], dim=1).to(torch.float16)[0]\n",
    "\n",
    "        # Move embeddings back to CPU for comparison and storage\n",
    "        precomputed_embedding = precomputed_embedding.cpu()\n",
    "        computed_embedding = computed_embedding.cpu()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Precomputed embedding: {precomputed_embedding}\")\n",
    "            print(f\"Computed embedding: {computed_embedding}\")\n",
    "\n",
    "        # Compute the difference\n",
    "        diff = F.mse_loss(precomputed_embedding, computed_embedding).item()\n",
    "        differences.append(diff)\n",
    "        if diff>1:\n",
    "            # os.remove(emebdding_path)\n",
    "            print(f\"remove {emebdding_path}, with diff {diff}\")\n",
    "            \n",
    "        if verbose:\n",
    "            print(f\"Difference (MSE): {diff}\\n\")\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(f\"All test indices: {all_test_indices}\")\n",
    "    print(f\"Average difference over {n_tests} tests: {sum(differences) / n_tests}\")\n",
    "    print(f\"Max difference: {max(differences)}\")\n",
    "    print(f\"Min difference: {min(differences)}\")\n",
    "\n",
    "start_time = time.time()\n",
    "# Usage example:\n",
    "compare_image_embeddings(model,processor,n_tests=1000, verbose=False)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Manually inspect single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-large/laion30m/1272.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [111]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load pre-computed embedding\u001b[39;00m\n\u001b[1;32m     14\u001b[0m emebdding_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-large/laion30m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43memebdding_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m precomputed_embedding \u001b[38;5;241m=\u001b[39m x[idx]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Compute embedding on-the-fly\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/openflamingo/lib/python3.9/site-packages/torch/serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.conda/envs/openflamingo/lib/python3.9/site-packages/torch/serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.conda/envs/openflamingo/lib/python3.9/site-packages/torch/serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-large/laion30m/1272.pt'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "device = 'cuda'\n",
    "file_idx = random.randint(0, 5000)\n",
    "idx = random.randint(0, 4096)  # Changed to 0-4095 to match the tensor size\n",
    "test_index = file_idx * 4096 + idx\n",
    "\n",
    "image_path = os.path.join(DATADIR['laion30m']['imagedir'], csv.iloc[test_index]['Image Path'])\n",
    "image = Image.open(image_path)\n",
    "\n",
    "\n",
    "# Load pre-computed embedding\n",
    "emebdding_path = f\"/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-large/laion30m/{file_idx}.pt\"\n",
    "\n",
    "x = torch.load(emebdding_path, weights_only=True)\n",
    "precomputed_embedding = x[idx].to(device)\n",
    "\n",
    "# Compute embedding on-the-fly\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "sequence_output = outputs[0]\n",
    "cls_token = sequence_output[:, 0]\n",
    "patch_tokens = sequence_output[:, 1:]\n",
    "computed_embedding = torch.cat([cls_token, patch_tokens.mean(dim=1)], dim=1).to(torch.float16)[0]\n",
    "\n",
    "# Move embeddings back to CPU for comparison and storage\n",
    "precomputed_embedding = precomputed_embedding.cpu()\n",
    "computed_embedding = computed_embedding.cpu()\n",
    "print(f\"image index {test_index}\")\n",
    "print(f\"Precomputed embedding: {precomputed_embedding}\")\n",
    "print(f\"Computed embedding: {computed_embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Visualize the image and its caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "# Get the caption and image path\n",
    "idx = random.randint(0, len(df))\n",
    "caption = df.iloc[idx]['caption']\n",
    "image_path = os.path.join(\"/home/mila/l/le.zhang/scratch/datasets/LAION\", df.iloc[idx]['Image Path'])\n",
    "\n",
    "# Open and display the image\n",
    "img = Image.open(image_path)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "\n",
    "# Add the caption at the bottom of the image\n",
    "plt.text(0.5, -0.05, caption, wrap=True, horizontalalignment='center', \n",
    "         verticalalignment='top', transform=plt.gca().transAxes)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import random\n",
    "x1 = torch.load('/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-large/dreamclipcc3m/0.pt')\n",
    "x2 = torch.load('/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-large/dreamclipcc3m1/0.pt').to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.randint(0, 1024)\n",
    "x1[idx],x2[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1955, 406, 1164, 881]\n"
     ]
    }
   ],
   "source": [
    "# randomly select 10000 samples from /home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-large/dreamclipcc3m and copy to /home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-large/validation, at\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "image_source_dir = '/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-large/laion30m'\n",
    "image_destination_dir = '/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/image_embedding/dinov2-large/validation'\n",
    "text_source_dir = '/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/text_embedding/gte-large-en-v1.5/laion30m_caption'\n",
    "text_destination_dir = '/home/mila/l/le.zhang/scratch/light_align/data/tensor_data/text_embedding/gte-large-en-v1.5/validation'\n",
    "\n",
    "# Create the destination directory if it doesn't exist\n",
    "os.makedirs(image_destination_dir, exist_ok=True)\n",
    "os.makedirs(text_destination_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "file_index = random.sample(range(len(files)), 4)\n",
    "print(file_index)\n",
    "# copy image files\n",
    "for file in file_index:\n",
    "    shutil.copy(os.path.join(image_source_dir, str(file)+'.pt'), image_destination_dir)\n",
    "\n",
    "# copy text files\n",
    "for file in file_index:\n",
    "    shutil.copy(os.path.join(text_source_dir, str(file)+'.pt'), text_destination_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 5., 9.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3],[4,5,6],[7,8,9]],dtype=torch.float32)\n",
    "torch.diagonal(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [7., 8., 9.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.diagonal(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openflamingo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
