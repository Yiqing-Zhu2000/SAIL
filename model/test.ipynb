{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, Dinov2Model\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "model = Dinov2Model.from_pretrained(\"facebook/dinov2-base\")\n",
    "\n",
    "inputs = image_processor([image,image], return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config._name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SiglipMLP(nn.Module):\n",
    "    def __init__(self, input_dim, intermediate_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.pre_norm = nn.LayerNorm(input_dim)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(input_dim, intermediate_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(intermediate_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.pre_norm(hidden_states)\n",
    "        hidden_states = hidden_states+self.proj(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class VLContrastHead(nn.Module):\n",
    "    def __init__(self, vision_dimesion, text_dimension, device, target_dimension=512, linear=False):\n",
    "        super(VLContrastHead, self).__init__()\n",
    "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.linear = linear\n",
    "        if self.linear:\n",
    "            self.vision_mapping_network = nn.Linear(vision_dimesion, target_dimension)\n",
    "            self.text_mapping_network = nn.Linear(text_dimension, target_dimension)\n",
    "        else:\n",
    "            # self.vision_mapping_network = SiglipMLP(vision_dimesion, target_dimension, target_dimension)\n",
    "            # self.text_mapping_network = SiglipMLP(text_dimension, target_dimension, target_dimension)\n",
    "            self.vision_mapping_network = nn.Linear(vision_dimesion, target_dimension)\n",
    "            self.text_mapping_network = nn.Linear(text_dimension, target_dimension)\n",
    "            self.mapping_network = SiglipMLP(target_dimension, target_dimension, target_dimension)\n",
    "\n",
    "        self.vision_layer_norm = nn.LayerNorm(vision_dimesion)\n",
    "        self.text_layer_norm = nn.LayerNorm(text_dimension)\n",
    "        self.logit_scale = nn.Parameter(torch.randn(1))\n",
    "        self.logit_bias = nn.Parameter(torch.randn(1))\n",
    "\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                torch.nn.init.ones_(module.weight)\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "        # Initialize logit_scale and logit_bias\n",
    "        logit_scale_init = torch.log(torch.tensor(10.0))\n",
    "        self.logit_scale.data.fill_(logit_scale_init)\n",
    "        self.logit_bias.data.fill_(torch.tensor(-10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = VLContrastHead(512, 512, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head.mapping_network.proj[2].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 尝试释放显存\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 查看显存使用情况\n",
    "print(torch.cuda.memory_allocated(0))\n",
    "print(torch.cuda.memory_reserved(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "class StarMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        output_dim: int,\n",
    "        intermediate_dim: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        intermediate_dim = intermediate_dim if intermediate_dim is not None else output_dim\n",
    "        self.Wa = nn.Linear(input_dim, input_dim, bias=False)\n",
    "        self.Wb = nn.Linear(input_dim, input_dim, bias=False)\n",
    "        self.g = nn.Linear(input_dim, output_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        a = self.Wa(x)  # N x d\n",
    "        b = self.Wb(x)  # N x d\n",
    "        x = torch.einsum('bij,bj->bi', torch.sigmoid(a.unsqueeze(-1) * b.unsqueeze(1)), x)\n",
    "        x = self.g(x)\n",
    "\n",
    "        assert not torch.isnan(x).any(), \"Output contains NaN\"\n",
    "        assert not torch.isinf(x).any(), \"Output contains infinite values\"\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networ = StarMLP(64, 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(16, 64)\n",
    "networ(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- configuration.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- modeling.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "path = 'Alibaba-NLP/gte-base-en-v1.5'\n",
    "device = torch.device('cuda')\n",
    "tokenzier = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    trust_remote_code=True,\n",
    "    unpad_inputs=True,\n",
    "    use_memory_efficient_attention=True,\n",
    ").to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\n",
    "    \"what is the capital of China?\",\n",
    "    \"how to implement quick sort in python?\",\n",
    "    \"Beijing\",\n",
    "    \"sorting algorithms\"\n",
    "]\n",
    "inputs = tokenzier(input_texts, max_length=8192, padding=True, truncation=True, return_tensors='pt')\n",
    "with torch.autocast(device_type=device.type, dtype=torch.float16):  # or bfloat16\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(**inputs.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2642,  0.5342, -0.3053,  ...,  0.5917,  0.3331,  0.6498],\n",
       "        [ 0.7925,  0.2668,  1.0746,  ..., -0.5660,  0.7037, -0.4509],\n",
       "        [-0.2670,  0.1429, -0.6497,  ...,  1.0327, -0.1330,  0.0383],\n",
       "        [ 0.3925, -0.7887,  0.3967,  ..., -0.8746, -0.0094, -0.3476]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/l/le.zhang/.conda/envs/openflamingo/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007d78ff206c4b10b72e43ad40eb807f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93a1d69588d4dd7a77222f5c9d16ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/813M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "model = AutoModel.from_pretrained(\"google/siglip-base-patch16-224\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([117.3308], grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.logit_scale.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4, 1], device='cuda:0')\n",
      "随机选择维度的结果: tensor([[-1.0714, -0.5365, -0.2856],\n",
      "        [ 0.5856,  0.8731,  0.0737],\n",
      "        [ 0.4370, -0.6297,  0.4586]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>) tensor([[ 0.7240, -1.4434,  0.6400],\n",
      "        [ 1.2815, -0.5635, -1.3642],\n",
      "        [ 0.4747, -0.4199, -0.5053]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def random_mask(self, v1, v2, r: float):\n",
    "    \"\"\"\n",
    "    随机从每个向量中挑选 k 个维度，同时保证梯度传导\n",
    "    :param v1: 第一个向量, 形状为 (n, d)\n",
    "    :param v2: 第二个向量, 形状为 (n, d)\n",
    "    :param r: 选择的维度比例（例如 0.5 表示选择一半的维度）\n",
    "    :return: 选中的维度对应的子向量 v1 和 v2\n",
    "    \"\"\"\n",
    "    assert v1.shape == v2.shape, \"两个张量的形状必须相同\"\n",
    "    n, d = v1.shape\n",
    "    k = int(d * r)\n",
    "    \n",
    "    # 获取 v1 所在设备\n",
    "    device = v1.device\n",
    "    \n",
    "    # 在相同的设备上生成随机索引\n",
    "    indices = torch.randperm(d, device=device)[:k]  # 从 d 中随机选择 k 个不重复的索引\n",
    "    print(indices)\n",
    "    # 选取对应维度\n",
    "    selected_v1 = v1[:, indices]  # 使用高级索引选择维度\n",
    "    selected_v2 = v2[:, indices]\n",
    "    \n",
    "    return selected_v1, selected_v2\n",
    "\n",
    "# 示例代码\n",
    "v1 = torch.randn(3, 5, device='cuda', requires_grad=True)  # 形状为 (5, 10) 的向量\n",
    "v2 = torch.randn(3, 5, device='cuda', requires_grad=True)  # 形状为 (5, 10) 的向量\n",
    "r = 0.6  # 选择50%的维度\n",
    "\n",
    "selected_v1, selected_v2 = random_mask(None, v1, v2, r)\n",
    "print(\"随机选择维度的结果:\", selected_v1, selected_v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([[ 0.2201,  2.1231],\n",
      "        [ 0.6620,  0.7037],\n",
      "        [ 0.4444,  1.6779],\n",
      "        [ 1.9595, -0.0101],\n",
      "        [-0.1047,  0.2060]]),\n",
      "indices=tensor([[1, 0],\n",
      "        [0, 0],\n",
      "        [1, 1],\n",
      "        [0, 1],\n",
      "        [1, 1]]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假设你的张量形状为 (n, d)\n",
    "n, d = 5, 4  # 示例数据\n",
    "tensor = torch.randn(n, d)\n",
    "\n",
    "# 定义分组数 m\n",
    "m = 2  # 比如将 d 维度分成 2 组\n",
    "\n",
    "# 检查 d 能否被 m 整除\n",
    "assert d % m == 0, \"d 维度必须能被 m 整除\"\n",
    "\n",
    "# 计算每组的大小\n",
    "group_size = d // m\n",
    "\n",
    "# 重塑张量，以便在 m 组上进行平均池化\n",
    "tensor_reshaped = tensor.view(n, m, group_size)\n",
    "\n",
    "# 对每组进行平均池化\n",
    "pooled_result = tensor_reshaped.max(dim=-1)\n",
    "\n",
    "print(pooled_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9637,  0.2201,  2.1231, -1.3038],\n",
       "        [ 0.6620, -0.1820,  0.7037,  0.0283],\n",
       "        [ 0.2498,  0.4444, -0.6127,  1.6779],\n",
       "        [ 1.9595, -0.2025, -2.1426, -0.0101],\n",
       "        [-2.3939, -0.1047, -0.5598,  0.2060]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.1231,  0.7037,  1.6779, -0.0101,  0.2060])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_v, max_index = tensor[:, 2:4].max(dim=-1)\n",
    "max_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/l/le.zhang/.conda/envs/openflamingo/lib/python3.9/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct:\n",
      "- tokenization_qwen.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct:\n",
      "- modeling_qwen.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e10e08551734428a0e4bc81b916f498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'unpad_inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m tokenzier \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(path, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43munpad_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_memory_efficient_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenzier([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest input\u001b[39m\u001b[38;5;124m'\u001b[39m], truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8192\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n",
      "File \u001b[0;32m~/.conda/envs/openflamingo/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:559\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/.conda/envs/openflamingo/lib/python3.9/site-packages/transformers/modeling_utils.py:3832\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3826\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   3827\u001b[0m     config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   3828\u001b[0m )\n\u001b[1;32m   3830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3831\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 3832\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3834\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3835\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'unpad_inputs'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "path = 'Alibaba-NLP/gte-Qwen2-1.5B-instruct'\n",
    "device = torch.device('cuda')\n",
    "tokenzier = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    trust_remote_code=True,\n",
    "    unpad_inputs=True,\n",
    "    use_memory_efficient_attention=True,\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "inputs = tokenzier(['test input'], truncation=True, max_length=8192, padding=True, return_tensors='pt')\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model(**inputs.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/network/scratch/l/le.zhang/hub\n"
     ]
    }
   ],
   "source": [
    "from transformers import TRANSFORMERS_CACHE\n",
    "print(TRANSFORMERS_CACHE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "device = torch.device('cuda')\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/vit-mae-large\",use_fast=True)\n",
    "model = AutoModel.from_pretrained(\"facebook/vit-mae-large\", attn_implementation=\"sdpa\", torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = image_processor(images=[image]*1024, return_tensors=\"pt\").to(model.device, dtype=torch.float16)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, Dinov2Model\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"huggingface/cats-image\", trust_remote_code=True)\n",
    "# image = dataset[\"test\"][\"image\"][0]\n",
    "# device = torch.device('cuda')\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "# model = Dinov2Model.from_pretrained(\"facebook/dinov2-base\", torch_dtype=torch.float16).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/mila/l/le.zhang/scratch/datasets/LAION/LAION30M/images/0000009/0007528.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image   \n\u001b[0;32m----> 2\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/mila/l/le.zhang/scratch/datasets/LAION/LAION30M/images/0000009/0007528.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m image\u001b[38;5;241m.\u001b[39msize\n",
      "File \u001b[0;32m~/.conda/envs/openflamingo/lib/python3.9/site-packages/PIL/Image.py:3236\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3233\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3236\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3237\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3239\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/mila/l/le.zhang/scratch/datasets/LAION/LAION30M/images/0000009/0007528.jpg'"
     ]
    }
   ],
   "source": [
    "from PIL import Image   \n",
    "image = Image.open('/home/mila/l/le.zhang/scratch/datasets/LAION/LAION30M/images/0000009/0007528.jpg')\n",
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 257, 768]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = image_processor(image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2306101/403876080.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weight = torch.load('/home/mila/l/le.zhang/scratch/light_align/logs/cc3m_gtendinoL_bs_65536_lion_mean_lr_1e-5_star7_d1024_scale50_negbias10/checkpoints/epoch_10.pt')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "weight = torch.load('/home/mila/l/le.zhang/scratch/light_align/logs/cc3m_gtendinoL_bs_65536_lion_mean_lr_1e-5_star7_d1024_scale50_negbias10/checkpoints/epoch_10.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 2048])\n",
      "torch.Size([4096, 2048])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([2048, 1024])\n",
      "torch.Size([2048, 1024])\n",
      "torch.Size([1024, 2048])\n"
     ]
    }
   ],
   "source": [
    "print(weight['state_dict']['vision_mapping_network.f1.weight'].shape)\n",
    "print(weight['state_dict']['vision_mapping_network.f2.weight'].shape)\n",
    "print(weight['state_dict']['vision_mapping_network.g.weight'].shape)\n",
    "\n",
    "print(weight['state_dict']['text_mapping_network.f1.weight'].shape)\n",
    "print(weight['state_dict']['text_mapping_network.f2.weight'].shape)\n",
    "print(weight['state_dict']['text_mapping_network.g.weight'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7246, -0.3332,  1.0531,  ...,  0.5195,  0.6481, -1.0260],\n",
      "        [-0.3381, -0.4691, -0.6043,  ...,  0.7669,  0.3860,  1.0222],\n",
      "        [-0.9226, -0.9556, -1.7079,  ...,  1.2693, -1.3151, -0.9646],\n",
      "        ...,\n",
      "        [ 0.8557,  0.8758,  0.9854,  ..., -0.4275, -0.0440,  0.2412],\n",
      "        [ 0.6166,  0.3915,  0.2303,  ..., -0.1609,  1.2541, -0.4990],\n",
      "        [-0.6820,  0.5480, -0.2736,  ..., -1.4394,  0.1189, -0.1482]])\n",
      "tensor([[-0.7246, -0.3332,  1.0531,  ...,  0.5195,  0.6481, -1.0260],\n",
      "        [-0.3381, -0.4691, -0.6043,  ...,  0.7669,  0.3860,  1.0222],\n",
      "        [-0.9226, -0.9556, -1.7079,  ...,  1.2693, -1.3151, -0.9646],\n",
      "        ...,\n",
      "        [ 0.8557,  0.8758,  0.9854,  ..., -0.4275, -0.0440,  0.2412],\n",
      "        [ 0.6166,  0.3915,  0.2303,  ..., -0.1609,  1.2541, -0.4990],\n",
      "        [-0.6820,  0.5480, -0.2736,  ..., -1.4394,  0.1189, -0.1482]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def z_score_normalize(features):\n",
    "    mean = features.mean(dim=0, keepdim=True)\n",
    "    std = features.std(dim=0, keepdim=True)\n",
    "    return (features - mean) / std\n",
    "\n",
    "features = torch.randn(10, 1024)\n",
    "features = z_score_normalize(features)\n",
    "print(features)\n",
    "print((features - features.mean(0)) / features.std(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openflamingo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
